# 第一部分：数学基础

## 第1章 线性代数

### 1.1 向量与矩阵运算

#### 1.1.1 向量基础

**向量定义**：
向量是具有大小和方向的数学对象，在AI中通常表示为n维空间中的点。

**向量的表示**：
```
行向量：v = [v₁, v₂, ..., vₙ]
列向量：v = [v₁, v₂, ..., vₙ]ᵀ
```

**向量运算**：
1. **向量加法**：
   ```
   u + v = [u₁ + v₁, u₂ + v₂, ..., uₙ + vₙ]
   ```

2. **标量乘法**：
   ```
   αv = [αv₁, αv₂, ..., αvₙ]
   ```

3. **点积（内积）**：
   ```
   u·v = u₁v₁ + u₂v₂ + ... + uₙvₙ = Σᵢuᵢvᵢ
   ```

4. **向量长度（范数）**：
   ```
   ||v|| = √(v₁² + v₂² + ... + vₙ²) = √(v·v)
   ```

**向量在AI中的应用**：
- **特征向量**：表示数据特征
- **权重向量**：神经网络权重
- **梯度向量**：优化算法中的梯度
- **词向量**：自然语言处理中的词表示

#### 1.1.2 矩阵运算

**矩阵定义**：
矩阵是m×n个元素按矩形排列的数学对象，记作A∈ℝᵐˣⁿ。

**矩阵运算**：
1. **矩阵加法**：
   ```
   C = A + B，其中cᵢⱼ = aᵢⱼ + bᵢⱼ
   ```

2. **矩阵乘法**：
   ```
   C = AB，其中cᵢⱼ = Σₖaᵢₖbₖⱼ
   ```

3. **矩阵转置**：
   ```
   Aᵀ的元素为(Aᵀ)ᵢⱼ = aⱼᵢ
   ```

4. **矩阵的迹**：
   ```
   tr(A) = Σᵢaᵢᵢ（对角元素之和）
   ```

**特殊矩阵**：
- **单位矩阵**：对角元素为1，其余为0
- **对角矩阵**：非对角元素为0
- **对称矩阵**：A = Aᵀ
- **正交矩阵**：AᵀA = I

#### 1.1.3 矩阵的逆与行列式

**行列式**：
行列式是方阵的标量值，记作det(A)或|A|。

**行列式性质**：
- det(AB) = det(A)det(B)
- det(Aᵀ) = det(A)
- det(αA) = αⁿdet(A)（n为矩阵阶数）

**矩阵的逆**：
矩阵A的逆A⁻¹满足AA⁻¹ = A⁻¹A = I。

**逆矩阵存在条件**：
- 方阵
- 行列式不为零（det(A) ≠ 0）
- 满秩

**逆矩阵计算**：
```
对于2×2矩阵：A = [[a, b], [c, d]]
A⁻¹ = (1/det(A))[[d, -b], [-c, a]]
```

#### 1.1.4 线性代数在AI中的应用

**线性变换**：
- **数据变换**：PCA降维
- **特征提取**：线性特征提取
- **图像处理**：图像变换

**方程组求解**：
- **线性回归**：正规方程求解
- **参数估计**：最小二乘法
- **优化问题**：约束优化

**特征值分解**：
- **主成分分析**：PCA
- **谱聚类**：图分割
- **PageRank**：网页排序

### 1.2 特征值与特征向量

#### 1.2.1 特征值问题定义

**特征值与特征向量**：
对于方阵A，如果存在非零向量v和标量λ使得Av = λv，则λ称为A的特征值，v称为对应的特征向量。

**特征方程**：
```
det(A - λI) = 0
```

**特征多项式**：
特征方程展开得到的多项式称为特征多项式。

#### 1.2.2 特征值计算

**特征值性质**：
- n阶方阵有n个特征值（计入重数）
- 实对称矩阵的特征值都是实数
- 特征值的和等于矩阵的迹
- 特征值的积等于行列式

**特征向量性质**：
- 对应不同特征值的特征向量线性无关
- 实对称矩阵的特征向量正交
- 特征向量可以归一化

**特征值分解**：
```
A = PDP⁻¹
其中D为对角矩阵（特征值），P为特征向量矩阵
```

#### 1.2.3 对称矩阵的特征分解

**实对称矩阵**：
实对称矩阵的特征分解具有优良性质：
- 特征值都是实数
- 特征向量正交
- 可以正交对角化

**谱定理**：
实对称矩阵可以表示为：
```
A = QΛQᵀ
其中Q为正交矩阵，Λ为对角矩阵
```

#### 1.2.4 特征值在AI中的应用

**主成分分析（PCA）**：
- 数据降维
- 特征提取
- 数据可视化

**谱聚类**：
- 图分割
- 社区发现
- 图像分割

**PageRank算法**：
- 网页排序
- 推荐系统
- 社交网络分析

### 1.3 矩阵分解

#### 1.3.1 特征值分解

**特征值分解**：
将矩阵分解为特征值和特征向量的形式：
```
A = PDP⁻¹
```

**适用条件**：
- 方阵
- 有n个线性无关的特征向量

**在AI中的应用**：
- PCA降维
- 数据压缩
- 特征提取

#### 1.3.2 奇异值分解（SVD）

**SVD定义**：
任意矩阵A∈ℝᵐˣⁿ可以分解为：
```
A = UΣVᵀ
其中U∈ℝᵐˣᵐ，Σ∈ℝᵐˣⁿ，V∈ℝⁿˣⁿ
```

**SVD性质**：
- U和V是正交矩阵
- Σ是对角矩阵，对角元素为奇异值
- 奇异值非负且按降序排列

**SVD计算**：
```
奇异值：σᵢ = √λᵢ，其中λᵢ为AᵀA的特征值
左奇异向量：U的列向量
右奇异向量：V的列向量
```

#### 1.3.3 其他矩阵分解

**QR分解**：
将矩阵分解为正交矩阵Q和上三角矩阵R：
```
A = QR
```

**LU分解**：
将矩阵分解为下三角矩阵L和上三角矩阵U：
```
A = LU
```

**Cholesky分解**：
对称正定矩阵的分解：
```
A = LLᵀ
```

#### 1.3.4 矩阵分解在AI中的应用

**数据压缩**：
- 图像压缩
- 数据降维
- 特征提取

**推荐系统**：
- 协同过滤
- 矩阵补全
- 个性化推荐

**自然语言处理**：
- 潜在语义分析
- 主题模型
- 文本表示

### 1.4 线性代数在深度学习中的应用

#### 1.4.1 神经网络中的线性代数

**前向传播**：
神经网络层的计算：
```
z = Wx + b
y = f(z)
```

**反向传播**：
梯度计算：
```
∂L/∂W = (∂L/∂y)(∂y/∂z)(∂z/∂W)
```

**批量处理**：
矩阵运算实现批量处理：
```
Z = WX + B
```

#### 1.4.2 优化算法中的线性代数

**梯度下降**：
参数更新：
```
W = W - α∇W
b = b - α∇b
```

**二阶优化**：
牛顿法：
```
W = W - H⁻¹∇W
```

#### 1.4.3 卷积运算

**卷积操作**：
图像卷积的矩阵表示：
```
y = W * x
```

**池化操作**：
最大池化、平均池化的矩阵运算。

#### 1.4.4 注意力机制

**注意力计算**：
注意力权重：
```
Attention(Q, K, V) = softmax(QKᵀ/√dₖ)V
```

**自注意力**：
自注意力机制的矩阵运算。

## 第2章 概率论与数理统计

### 2.1 概率基础

#### 2.1.1 概率的基本概念

**概率定义**：
概率是事件发生可能性的数值度量，取值范围[0, 1]。

**概率的公理化定义**：
1. 非负性：P(A) ≥ 0
2. 规范性：P(Ω) = 1
3. 可列可加性：对互斥事件A₁, A₂, ...，有P(∪Aᵢ) = ΣP(Aᵢ)

**条件概率**：
在事件B发生的条件下，事件A发生的概率：
```
P(A|B) = P(A∩B)/P(B)
```

**贝叶斯定理**：
```
P(A|B) = P(B|A)P(A)/P(B)
```

#### 2.1.2 随机变量

**随机变量定义**：
随机变量是随机试验结果的数值函数。

**离散随机变量**：
取值有限或可数的随机变量。

**连续随机变量**：
取值连续的随机变量。

**随机变量的函数**：
Y = g(X)也是随机变量。

#### 2.1.3 概率分布

**离散分布**：
- **伯努利分布**：单次试验成功概率
- **二项分布**：n次独立试验成功次数
- **泊松分布**：单位时间内事件发生次数
- **几何分布**：首次成功所需试验次数

**连续分布**：
- **均匀分布**：区间内等概率
- **正态分布**：自然界最常见的分布
- **指数分布**：等待时间分布
- **伽马分布**：等待多个事件时间

**分布函数**：
累积分布函数（CDF）：
```
F(x) = P(X ≤ x)
```

**概率密度函数**：
连续随机变量的PDF：
```
f(x) = dF(x)/dx
```

#### 2.1.4 期望与方差

**期望（均值）**：
随机变量的期望值：
```
离散：E[X] = Σx·P(X=x)
连续：E[X] = ∫x·f(x)dx
```

**方差**：
随机变量的离散程度：
```
Var(X) = E[(X-E[X])²] = E[X²] - (E[X])²
```

**标准差**：
方差的平方根：
```
σ = √Var(X)
```

**协方差**：
两个随机变量的相关性：
```
Cov(X,Y) = E[(X-E[X])(Y-E[Y])]
```

**相关系数**：
标准化的协方差：
```
ρ = Cov(X,Y)/(σₓσᵧ)
```

### 2.2 随机变量与分布

#### 2.2.1 多维随机变量

**联合分布**：
多个随机变量的联合概率分布。

**边缘分布**：
从联合分布得到单个变量的分布。

**条件分布**：
给定某些变量值时其他变量的分布。

**独立性**：
随机变量X和Y独立当且仅当：
```
P(X=x, Y=y) = P(X=x)P(Y=y)
```

#### 2.2.2 重要概率分布

**正态分布**：
概率密度函数：
```
f(x) = (1/√(2πσ²))exp(-(x-μ)²/(2σ²))
```

**多元正态分布**：
d维正态分布：
```
X ~ N(μ, Σ)
其中μ为均值向量，Σ为协方差矩阵
```

**学生t分布**：
比正态分布更厚的尾部，用于小样本推断。

**卡方分布**：
多个独立标准正态变量平方和的分布。

**F分布**：
两个卡方分布比值的分布。

#### 2.2.3 分布的性质

**矩**：
- **原点矩**：E[Xᵏ]
- **中心矩**：E[(X-E[X])ᵏ]
- **偏度**：三阶标准化矩
- **峰度**：四阶标准化矩

**矩生成函数**：
```
M(t) = E[eᵗˣ]
```

**特征函数**：
```
φ(t) = E[eⁱᵗˣ]
```

#### 2.2.4 分布在AI中的应用

**高斯分布**：
- **高斯混合模型**：聚类
- **高斯过程**：回归和分类
- **贝叶斯推断**：先验分布

**伯努利分布**：
- **逻辑回归**：二分类
- **神经网络**：输出层激活
- **强化学习**：动作选择

**多项分布**：
- **朴素贝叶斯**：文本分类
- **主题模型**：文本主题
- **强化学习**：策略分布

### 2.3 统计推断

#### 2.3.1 参数估计

**点估计**：
用样本统计量估计总体参数。

**最大似然估计（MLE）**：
```
θ̂ = argmax θ L(θ|X) = argmax θ P(X|θ)
```

**最大后验估计（MAP）**：
```
θ̂ = argmax θ P(θ|X) = argmax θ P(X|θ)P(θ)
```

**贝叶斯估计**：
考虑参数的不确定性：
```
P(θ|X) ∝ P(X|θ)P(θ)
```

#### 2.3.2 置信区间

**置信区间定义**：
参数真值以指定概率落入的区间。

**置信水平**：
1-α，通常取95%或99%。

**置信区间构造**：
- **正态分布**：μ的置信区间
- **比例估计**：p的置信区间
- **方差估计**：σ²的置信区间

**置信区间解释**：
95%置信区间意味着在重复抽样中，有95%的区间包含真值。

#### 2.3.3 假设检验

**假设检验步骤**：
1. 建立原假设H₀和备择假设H₁
2. 选择检验统计量
3. 确定显著性水平α
4. 计算p值
5. 做出决策

**检验统计量**：
- **z检验**：已知方差时均值检验
- **t检验**：未知方差时均值检验
- **χ²检验**：方差检验、拟合优度检验
- **F检验**：方差比较检验

**p值**：
在原假设下，观察到的检验统计量更极端的概率。

**第一类错误**：弃真错误（α）
**第二类错误**：取伪错误（β）

#### 2.3.4 贝叶斯推断

**贝叶斯定理**：
```
P(θ|X) = P(X|θ)P(θ)/P(X)
```

**先验分布**：P(θ)
**似然函数**：P(X|θ)
**后验分布**：P(θ|X)

**贝叶斯更新**：
随着数据累积更新后验分布。

**贝叶斯预测**：
```
P(xₙ₊₁|x₁, ..., xₙ) = ∫P(xₙ₊₁|θ)P(θ|x₁, ..., xₙ)dθ
```

### 2.4 假设检验

#### 2.4.1 基本概念

**假设类型**：
- **简单假设**：参数取特定值
- **复合假设**：参数取值集合

**检验类型**：
- **单边检验**：H₁: θ > θ₀或θ < θ₀
- **双边检验**：H₁: θ ≠ θ₀

**功效函数**：
1 - β(θ)，拒绝错误原假设的概率。

#### 2.4.2 常用检验方法

**t检验**：
- **单样本t检验**：检验均值是否等于给定值
- **两样本t检验**：检验两个样本均值是否相等
- **配对t检验**：检验配对数据均值差异

**方差分析（ANOVA）**：
- **单因素ANOVA**：检验多个组均值是否相等
- **双因素ANOVA**：检验两个因素的影响

**非参数检验**：
- **Wilcoxon秩和检验**：两独立样本
- **Kruskal-Wallis检验**：多个独立样本
- **Friedman检验**：多个相关样本

#### 2.4.3 多重检验问题

**多重检验**：
同时进行多个假设检验。

**问题**：
第一类错误概率累积。

**校正方法**：
- **Bonferroni校正**：α' = α/m
- **FDR校正**：控制错误发现率
- **Holm校正**：逐步拒绝法

#### 2.4.4 统计检验在AI中的应用

**特征选择**：
- **t检验**：选择重要特征
- **ANOVA**：多组特征比较
- **非参数检验**：非正态数据特征选择

**模型比较**：
- **配对t检验**：比较两个模型性能
- **ANOVA**：比较多个模型性能
- **非参数检验**：非正态性能比较

**A/B测试**：
- **比例检验**：转化率比较
- **均值检验**：指标均值比较
- **非参数检验**：非正态指标比较

## 第3章 最优化理论

### 3.1 无约束优化

#### 3.1.1 优化问题基本概念

**优化问题形式**：
```
min f(x)
s.t. x ∈ ℝⁿ
```

**最优解类型**：
- **全局最优**：在整个定义域内的最优解
- **局部最优**：在某个邻域内的最优解

**最优性条件**：
- **一阶条件**：∇f(x*) = 0
- **二阶条件**：Hessian矩阵正定

#### 3.1.2 梯度下降法

**梯度下降算法**：
```
xₖ₊₁ = xₖ - αₖ∇f(xₖ)
其中αₖ为步长（学习率）
```

**学习率选择**：
- **固定学习率**：常数步长
- **线搜索**：最优步长
- **自适应学习率**：根据梯度调整

**收敛性**：
- 对于凸函数，梯度下降收敛到全局最优
- 对于非凸函数，收敛到局部最优

**梯度下降变体**：
- **批量梯度下降**：使用全部数据
- **随机梯度下降（SGD）**：使用单个样本
- **小批量梯度下降**：使用小批量数据

#### 3.1.3 牛顿法

**牛顿法算法**：
```
xₖ₊₁ = xₖ - [Hf(xₖ)]⁻¹∇f(xₖ)
```

**收敛速度**：
- 二阶收敛（接近最优解时）
- 需要计算Hessian矩阵及其逆

**拟牛顿法**：
- **BFGS**：逼近Hessian矩阵
- **L-BFGS**：内存有限的BFGS
- **DFP**：另一种拟牛顿法

#### 3.1.4 共轭梯度法

**共轭方向**：
两个方向d和q共轭如果dᵀHq = 0。

**共轭梯度算法**：
```
dₖ₊₁ = -∇f(xₖ₊₁) + βₖdₖ
```

**优点**：
- 不需要存储Hessian矩阵
- 收敛速度快于梯度下降
- 适合大规模优化问题

### 3.2 约束优化

#### 3.2.1 等式约束优化

**拉格朗日乘数法**：
```
min f(x)
s.t. h(x) = 0

拉格朗日函数：L(x,λ) = f(x) + λᵀh(x)
```

**KKT条件**：
- ∇ₓL = 0
- h(x) = 0
- λ无约束

#### 3.2.2 不等式约束优化

**不等式约束问题**：
```
min f(x)
s.t. g(x) ≤ 0
```

**KKT条件**：
- ∇ₓL = 0
- g(x) ≤ 0
- λ ≥ 0
- λᵢgᵢ(x) = 0（互补松弛条件）

#### 3.2.3 罚函数法

**外罚函数法**：
将约束转化为罚项：
```
min f(x) + ρ·Σ[max(0, gᵢ(x))]²
```

**内罚函数法**：
使用障碍函数保持可行性：
```
min f(x) - ρ·Σlog(-gᵢ(x))
```

#### 3.2.4 投影梯度法

**投影操作**：
将解投影到可行域：
```
xₖ₊₁ = P_C[xₖ - αₖ∇f(xₖ)]
```

**应用场景**：
- 简单约束（如非负约束）
- 稀疏约束
- 范数球约束

### 3.3 凸优化基础

#### 3.3.1 凸集与凸函数

**凸集定义**：
集合C是凸集如果对于任意x,y∈C和λ∈[0,1]，有：
```
λx + (1-λ)y ∈ C
```

**凸函数定义**：
函数f是凸函数如果定义域是凸集且：
```
f(λx + (1-λ)y) ≤ λf(x) + (1-λ)f(y)
```

**凸函数判别**：
- 一阶条件：f(y) ≥ f(x) + ∇f(x)ᵀ(y-x)
- 二阶条件：Hessian矩阵半正定

#### 3.3.2 凸优化问题

**凸优化问题形式**：
```
min f(x)
s.t. gᵢ(x) ≤ 0, i = 1,...,m
     hᵢ(x) = 0, i = 1,...,p
```

其中f和gᵢ是凸函数，hᵢ是仿射函数。

**最优性条件**：
- KKT条件既是必要条件也是充分条件
- 局部最优就是全局最优
- 对偶问题有强对偶性

#### 3.3.3 对偶理论

**拉格朗日对偶**：
```
对偶函数：g(λ,ν) = infₓ L(x,λ,ν)
对偶问题：max g(λ,ν) s.t. λ ≥ 0
```

**对偶间隙**：
原始最优值与对偶最优值的差。

**强对偶性**：
对偶间隙为零的条件。

#### 3.3.4 常见凸优化问题

**线性规划**：
目标函数和约束都是线性的。

**二次规划**：
目标函数是二次的，约束是线性的。

**半定规划**：
约束包含矩阵正定性。

**锥优化**：
约束涉及凸锥。

### 3.4 优化算法在AI中的应用

#### 3.4.1 神经网络训练

**损失函数优化**：
- **均方误差**：MSE = (1/n)Σ(y-ŷ)²
- **交叉熵**：CE = -Σy log(ŷ)
- **Hinge损失**：用于SVM

**优化器选择**：
- **SGD**：简单高效
- **Adam**：自适应学习率
- **RMSprop**：解决RNN训练问题
- **Adagrad**：适合稀疏特征

#### 3.4.2 正则化优化

**L1正则化**：
```
min L(θ) + λ||θ||₁
```

**L2正则化**：
```
min L(θ) + λ||θ||₂²
```

**弹性网络**：
```
min L(θ) + λ₁||θ||₁ + λ₂||θ||₂²
```

#### 3.4.3 约束优化应用

**稀疏约束**：
- L1正则化产生稀疏解
- 特征选择
- 压缩感知

**非负约束**：
- 非负矩阵分解
- 概率约束（概率和为1）

#### 3.4.4 分布式优化

**数据并行**：
- 各节点计算本地梯度
- 梯度聚合
- 同步更新

**模型并行**：
- 模型分割到不同节点
- 前向/反向传播
- 梯度交换

**异步优化**：
- 异步SGD
- 参数服务器架构
- 容错机制