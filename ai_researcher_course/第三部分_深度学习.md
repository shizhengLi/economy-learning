# 第三部分：深度学习

## 第7章 神经网络基础

### 7.1 感知器与多层感知器

#### 7.1.1 感知器模型

**感知器定义**：
感知器是最简单的神经网络模型，用于二元分类。

**数学模型**：
```
输入：x = [x₁, x₂, ..., xₙ]ᵀ
权重：w = [w₁, w₂, ..., wₙ]ᵀ
偏置：b
输出：y = f(w·x + b)
其中f为激活函数
```

**感知器学习算法**：
```
1. 初始化权重w和偏置b
2. 对于每个训练样本(x, y)：
   - 计算预测：ŷ = sign(w·x + b)
   - 如果预测错误：w = w + η(y - ŷ)x, b = b + η(y - ŷ)
3. 重复直到收敛或达到最大迭代次数
```

**感知器的局限性**：
- 只能解决线性可分问题
- 无法解决XOR等非线性问题
- 收敛性依赖于数据可分性

#### 7.1.2 多层感知器（MLP）

**MLP结构**：
```
输入层：接收原始特征
隐藏层：提取高级特征
输出层：产生最终预测
```

**前向传播**：
```
对于层l：
    z⁽ˡ⁾ = W⁽ˡ⁾a⁽ˡ⁻¹⁾ + b⁽ˡ⁾
    a⁽ˡ⁾ = f⁽ˡ⁾(z⁽ˡ⁾)
其中f⁽ˡ⁾为第l层的激活函数
```

**网络容量**：
- **层数**：网络深度
- **每层神经元数**：网络宽度
- **参数数量**：决定模型复杂度

#### 7.1.3 激活函数

**Sigmoid函数**：
```
f(x) = 1 / (1 + e⁻ˣ)
f'(x) = f(x)(1 - f(x))
特点：输出范围(0,1)，存在梯度消失
```

**Tanh函数**：
```
f(x) = (eˣ - e⁻ˣ) / (eˣ + e⁻ˣ)
f'(x) = 1 - f(x)²
特点：输出范围(-1,1)，零中心化
```

**ReLU函数**：
```
f(x) = max(0, x)
f'(x) = 1 if x > 0, 0 otherwise
特点：计算简单，缓解梯度消失
```

**Leaky ReLU**：
```
f(x) = x if x > 0, αx otherwise
其中α为小的正数（如0.01）
```

**ELU函数**：
```
f(x) = x if x > 0, α(eˣ - 1) otherwise
```

**Softmax函数**：
```
f(xᵢ) = eˣᵢ / Σⱼ eˣⱼ
用于多分类问题的输出层
```

#### 7.1.4 神经网络的表示能力

**通用近似定理**：
具有足够隐藏单元的单隐藏层神经网络可以以任意精度近似任何连续函数。

**网络深度与宽度**：
- **深度网络**：更参数高效，更好的特征层次
- **宽度网络**：更容易训练，更好的并行性
- **深度与宽度的权衡**：根据任务选择

**网络的容量控制**：
- **正则化**：L1、L2正则化
- **Dropout**：随机丢弃神经元
- **早停**：基于验证集性能停止训练
- **权重衰减**：限制权重大小

### 7.2 反向传播算法

#### 7.2.1 反向传播原理

**反向传播概述**：
反向传播是一种高效计算神经网络梯度的算法，基于链式法则。

**损失函数**：
```
均方误差：L = (1/2)Σᵢ(yᵢ - ŷᵢ)²
交叉熵：L = -Σᵢ yᵢ log(ŷᵢ)
```

**链式法则**：
对于复合函数f(g(x))，导数为：
```
df/dx = (df/dg) × (dg/dx)
```

#### 7.2.2 反向传播推导

**输出层梯度**：
```
对于输出层L：
    δ⁽ᴸ⁾ = ∂L/∂z⁽ᴸ⁾ = ∂L/∂a⁽ᴸ⁾ ⊙ f'(z⁽ᴸ⁾)
    ∂L/∂W⁽ᴸ⁾ = δ⁽ᴸ⁾(a⁽ᴸ⁻¹⁾)ᵀ
    ∂L/∂b⁽ᴸ⁾ = δ⁽ᴸ⁾
```

**隐藏层梯度**：
```
对于隐藏层l：
    δ⁽ˡ⁾ = ((W⁽ˡ⁺¹⁾)ᵀδ⁽ˡ⁺¹⁾) ⊙ f'(z⁽ˡ⁾)
    ∂L/∂W⁽ˡ⁾ = δ⁽ˡ⁾(a⁽ˡ⁻¹⁾)ᵀ
    ∂L/∂b⁽ˡ⁾ = δ⁽ˡ⁾
```

**反向传播算法步骤**：
```
1. 前向传播：计算各层的激活值
2. 计算输出层误差
3. 反向传播误差到各层
4. 计算各参数的梯度
5. 更新参数
```

#### 7.2.3 实现技巧

**数值稳定性**：
- **梯度裁剪**：防止梯度爆炸
- **权重初始化**：合适的初始化策略
- **批归一化**：标准化层输入

**矩阵运算优化**：
- **向量化**：使用矩阵运算代替循环
- **并行计算**：利用GPU并行性
- **内存优化**：减少内存访问

**调试技巧**：
- **梯度检查**：数值梯度验证
- **可视化**：监控训练过程
- **中间输出**：检查各层输出

#### 7.2.4 反向传播的变体

**随机梯度下降（SGD）**：
```
W = W - η∇W
其中η为学习率
```

**带动量的SGD**：
```
v = βv + (1-β)∇W
W = W - ηv
```

**Adam优化器**：
```
m = β₁m + (1-β₁)∇W
v = β₂v + (1-β₂)(∇W)²
m̂ = m/(1-β₁ᵗ)
v̂ = v/(1-β₂ᵗ)
W = W - ηm̂/(√v̂ + ε)
```

### 7.3 网络训练技巧

#### 7.3.1 权重初始化

**Xavier初始化**：
```
对于均匀分布：W ~ U[-6/√(n_in + n_out), 6/√(n_in + n_out)]
对于正态分布：W ~ N(0, 2/(n_in + n_out))
适合Sigmoid、Tanh激活函数
```

**He初始化**：
```
W ~ N(0, 2/n_in)
适合ReLU激活函数
```

**预训练**：
- **无监督预训练**：使用无监督学习初始化
- **迁移学习**：使用预训练模型权重
- **微调**：在目标任务上进一步训练

#### 7.3.2 正则化技术

**L1正则化**：
```
L_total = L_data + λΣ|W|
```

**L2正则化**：
```
L_total = L_data + (λ/2)ΣW²
```

**Dropout**：
```
训练时：以概率p随机丢弃神经元
测试时：所有神经元都使用，输出乘以p
```

**批归一化（Batch Normalization）**：
```
z = Wx + b
μ = E[z]  # 小批量均值
σ² = Var[z]  # 小批量方差
ẑ = (z - μ)/√(σ² + ε)
y = γẑ + β  # 缩放和平移
```

**层归一化（Layer Normalization）**：
```
对每个样本的所有特征进行归一化
```

#### 7.3.3 学习率调度

**固定学习率**：
- 简单但可能不是最优
- 需要手动调整

**学习率衰减**：
```
η(t) = η₀ / (1 + decay_rate × t)
```

**步长衰减**：
```
每隔一定步数将学习率乘以一个因子
```

**余弦退火**：
```
η(t) = η_min + (η_max - η_min)(1 + cos(πt/T))/2
```

**循环学习率**：
```
学习率在最小值和最大值之间周期性变化
```

#### 7.3.4 早停与验证

**早停策略**：
```
1. 监控验证集性能
2. 如果验证性能在patience个epoch内没有改善
3. 停止训练，恢复最佳权重
```

**验证策略**：
- **Hold-out验证**：简单的训练-验证-测试划分
- **K折交叉验证**：更可靠的性能评估
- **分层交叉验证**：保持类别分布

**模型选择**：
- **基于验证集**：选择验证性能最好的模型
- **基于测试集**：最终评估使用测试集
- **集成方法**：结合多个模型

### 7.4 深度学习的应用

#### 7.4.1 计算机视觉

**图像分类**：
- **CNN架构**：AlexNet、VGG、ResNet
- **迁移学习**：使用预训练模型
- **数据增强**：旋转、缩放、翻转等

**目标检测**：
- **Two-stage方法**：R-CNN系列
- **One-stage方法**：YOLO、SSD
- **Anchor-based vs Anchor-free**

**图像分割**：
- **语义分割**：FCN、U-Net
- **实例分割**：Mask R-CNN
- **全景分割**：Panoptic FPN

#### 7.4.2 自然语言处理

**文本分类**：
- **词嵌入**：Word2Vec、GloVe
- **RNN/LSTM**：序列建模
- **CNN**：文本CNN
- **Transformer**：BERT、GPT

**序列标注**：
- **命名实体识别**：NER
- **词性标注**：POS Tagging
- **分词**：Word Segmentation

**机器翻译**：
- **Seq2Seq**：编码器-解码器架构
- **Attention**：注意力机制
- **Transformer**：完全基于注意力

#### 7.4.3 推荐系统

**协同过滤**：
- **矩阵分解**：SVD、MF
- **深度矩阵分解**：神经网络矩阵分解
- **图神经网络**：知识图谱推荐

**内容推荐**：
- **深度内容推荐**：DSSM
- **多模态推荐**：融合多种信息
- **序列推荐**：R4Rec、SASRec

**强化学习推荐**：
- **DRL推荐**：深度强化学习
- **多臂老虎机**：UCB、Thompson Sampling
- **上下文老虎机**：LinUCB

#### 7.4.4 时序预测

**时间序列预测**：
- **RNN/LSTM**：序列建模
- **Transformer**：时序Transformer
- **TCN**：时间卷积网络
- **N-BEATS**：基于深度学习的时序预测

**异常检测**：
- **自编码器**：基于重构误差
- **GAN**：生成对抗网络
- **LSTM-AE**：LSTM自编码器
- **孤立森林**：Isolation Forest

**金融预测**：
- **股价预测**：LSTM、GRU
- **风险预测**：信用评分、欺诈检测
- **经济指标预测**：GDP、CPI等

## 第8章 卷积神经网络

### 8.1 CNN架构

#### 8.1.1 卷积层

**卷积操作**：
```
输入：I ∈ ℝᴴˣʷˣᶜ
卷积核：K ∈ ℝᵏˣᵏˣᶜˣᶜ'
输出：O ∈ ℝᴴ'ˣʷ'ˣᶜ'
其中：H' = (H - k + 2p)/s + 1
      W' = (W - k + 2p)/s + 1
```

**卷积的数学表达**：
```
O[i,j,m] = Σᶜ Σₖ Σₗ I[i+k,j+l,c] × K[k,l,c,m]
其中k、l为卷积核的行列索引
```

**参数共享**：
- 同一个卷积核在输入的不同位置使用相同的权重
- 大幅减少参数数量
- 提高平移不变性

**局部连接**：
- 每个输出单元只连接到输入的一个局部区域
- 捕获局部特征
- 减少连接数量

#### 8.1.2 池化层

**最大池化**：
```
在感受野内取最大值
O[i,j] = max_{k,l∈R} I[i+k,j+l]
其中R为感受野区域
```

**平均池化**：
```
在感受野内取平均值
O[i,j] = (1/|R|) Σ_{k,l∈R} I[i+k,j+l]
```

**全局池化**：
- **全局平均池化**：对整个特征图取平均
- **全局最大池化**：对整个特征图取最大值

**池化的作用**：
- 减少空间维度
- 提供平移不变性
- 控制过拟合

#### 8.1.3 激活函数

**ReLU在CNN中的应用**：
```
f(x) = max(0, x)
计算简单，缓解梯度消失
```

**Leaky ReLU**：
```
f(x) = x if x > 0, αx otherwise
解决ReLU的"死亡"问题
```

**ELU**：
```
f(x) = x if x > 0, α(eˣ - 1) otherwise
接近零均值，减少偏移
```

**SELU**：
```
自归一化神经网络，可以自动归一化
```

#### 8.1.4 经典CNN架构

**LeNet-5**：
- **输入**：32×32灰度图像
- **架构**：卷积-池化-卷积-池化-全连接
- **应用**：手写数字识别

**AlexNet**：
- **创新**：ReLU、Dropout、数据增强
- **架构**：5个卷积层 + 3个全连接层
- **影响**：深度学习复兴的标志

**VGGNet**：
- **特点**：使用小卷积核（3×3）
- **架构**：VGG16、VGG19
- **优势**：结构简单、效果好

**ResNet**：
- **创新**：残差连接
- **公式**：F(x) + x
- **优势**：解决深层网络梯度消失

### 8.2 CNN的核心组件

#### 8.2.1 卷积的变体

**转置卷积（反卷积）**：
```
用于上采样，常用于生成模型
O[i,j] = Σₖ Σₗ K[i-k,j-l] × I[k,l]
```

**空洞卷积**：
```
在卷积核元素之间插入空洞
感受野：R = (k-1)×r + 1
其中r为空洞率
```

**分组卷积**：
```
将输入通道分成g组，每组单独卷积
参数量减少为1/g
```

**深度可分离卷积**：
```
深度卷积 + 点卷积
大幅减少计算量
```

#### 8.2.2 批归一化与层归一化

**批归一化在CNN中的应用**：
```
对每个特征图在小批量内归一化
μ = (1/N)Σᵢ xᵢ
σ² = (1/N)Σᵢ (xᵢ - μ)²
x̂ = (x - μ)/√(σ² + ε)
y = γx̂ + β
```

**层归一化**：
```
对每个样本的所有特征归一化
适合小批量或在线学习
```

**实例归一化**：
```
对每个样本的每个特征图归一化
常用于风格迁移
```

#### 8.2.3 注意力机制

**空间注意力**：
```
学习特征图不同空间位置的重要性
A = σ(f₁(avg_pool(x)) + f₂(max_pool(x)))
x' = A ⊙ x
```

**通道注意力**：
```
学习不同通道的重要性
M = σ(f₂(ReLU(f₁(avg_pool(x))) + f₂(ReLU(f₁(max_pool(x)))))
x' = M ⊙ x
```

**自注意力**：
```
Q = W_q × x
K = W_k × x
V = W_v × x
A = softmax(QKᵀ/√d)
x' = AV
```

#### 8.2.4 残差连接与跳跃连接

**残差连接**：
```
F(x) + x
解决深层网络梯度消失
```

**密集连接**：
```
DenseNet：每层连接所有前面层
特征复用，参数高效
```

**Inception模块**：
```
GoogLeNet：并行使用不同大小的卷积核
多尺度特征提取
```

### 8.3 CNN的训练优化

#### 8.3.1 数据增强

**几何变换**：
- **翻转**：水平翻转、垂直翻转
- **旋转**：随机旋转
- **缩放**：随机缩放
- **裁剪**：随机裁剪
- **平移**：随机平移

**颜色变换**：
- **亮度调整**：随机亮度变化
- **对比度调整**：随机对比度变化
- **色调调整**：随机色调变化
- **饱和度调整**：随机饱和度变化

**高级增强**：
- **Cutout**：随机遮挡区域
- **Mixup**：线性混合两个样本
- **CutMix**：混合图像和标签
- **AutoAugment**：自动学习增强策略

**增强策略**：
- **在线增强**：训练时实时增强
- **离线增强**：预先生成增强数据
- **策略搜索**：自动搜索最佳增强策略

#### 8.3.2 迁移学习

**迁移学习的类型**：
- **特征提取**：冻结预训练层，只训练顶层
- **微调**：解冻部分层进行训练
- **自适应微调**：不同层使用不同学习率

**预训练模型**：
- **ImageNet预训练**：大规模图像分类
- **自监督预训练**：SimCLR、MoCo
- **多任务预训练**：同时训练多个任务

**迁移策略**：
```
1. 选择预训练模型
2. 替换分类头
3. 冻结部分层
4. 训练新层
5. 逐步解冻微调
```

#### 8.3.3 分布式训练

**数据并行**：
```
每个GPU处理不同的数据子集
同步梯度更新
```

**模型并行**：
```
将模型的不同层分配到不同GPU
适合超大模型
```

**混合并行**：
```
结合数据并行和模型并行
适用于超大规模训练
```

**分布式训练框架**：
- **Horovod**：基于MPI的分布式训练
- **PyTorch DDP**：PyTorch分布式数据并行
- **TensorFlow DistributionStrategy**：TensorFlow分布式策略

#### 8.3.4 模型压缩

**剪枝**：
```
移除不重要的连接或神经元
结构化剪枝 vs 非结构化剪枝
```

**量化**：
```
降低权重的精度
32位浮点 → 16位浮点 → 8位整数
```

**知识蒸馏**：
```
大模型（教师）教小模型（学生）
L = αL_ce + (1-α)L_kd
```

**神经网络架构搜索（NAS）**：
```
自动搜索最优网络架构
强化学习、进化算法、梯度优化
```

### 8.4 CNN的高级应用

#### 8.4.1 目标检测

**Two-stage方法**：
```
R-CNN → Fast R-CNN → Faster R-CNN
第一步：生成候选区域
第二步：分类和回归
```

**One-stage方法**：
```
YOLO、SSD、RetinaNet
直接预测边界框和类别
速度快，精度略低
```

**Anchor-based方法**：
```
预定义多个anchor box
学习对anchor的偏移
```

**Anchor-free方法**：
```
FCOS、CenterNet、CornerNet
直接预测边界框
```

#### 8.4.2 图像分割

**语义分割**：
```
像素级分类
FCN、U-Net、DeepLab
```

**实例分割**：
```
同时检测和分割
Mask R-CNN
```

**全景分割**：
```
结合语义分割和实例分割
Panoptic FPN
```

**分割的评估指标**：
```
mIoU（平均交并比）
Pixel Accuracy
F1 Score
```

#### 8.4.3 生成模型

**生成对抗网络（GAN）**：
```
生成器：G(z) → 生成图像
判别器：D(x) → 判断真假
对抗训练：min_G max_D V(D,G)
```

**VAE（变分自编码器）**：
```
编码器：q(z|x) → 编码为隐变量
解码器：p(x|z) → 重建图像
ELBO：E[log p(x|z)] - KL(q(z|x)||p(z))
```

**扩散模型**：
```
前向过程：逐步添加噪声
反向过程：逐步去噪声
```

#### 8.4.4 多模态学习

**图文匹配**：
```
CLIP：对比语言-图像预训练
ALIGN：大规模图文对齐
```

**视觉问答（VQA）**：
```
给定图像和问题，回答问题
需要理解图像和文本
```

**图像描述生成**：
```
生成图像的自然语言描述
CNN + RNN/LSTM架构
```

**跨模态检索**：
```
用一个模态查询另一个模态
文本检索图像、图像检索文本
```

## 第9章 循环神经网络

### 9.1 RNN基础

#### 9.1.1 RNN的基本概念

**循环神经网络的动机**：
传统神经网络无法处理序列数据，RNN引入记忆机制来处理时序信息。

**RNN的基本结构**：
```
隐藏状态：hₜ = f(Wₕₕhₜ₋₁ + Wₓₕxₜ + bₕ)
输出：yₜ = f(Wₕᵧhₜ + bᵧ)
其中f为激活函数（通常为tanh）
```

**RNN的计算图**：
- **时间步展开**：将RNN在时间维度上展开
- **参数共享**：所有时间步共享相同的权重
- **序列处理**：可以处理变长序列

**RNN的变体**：
- **Elman网络**：标准RNN
- **Jordan网络**：输出反馈到隐藏层
- **双向RNN**：同时考虑过去和未来信息

#### 9.1.2 RNN的训练

**反向传播通过时间（BPTT）**：
```
展开计算图
计算每个时间步的梯度
按时间反向传播梯度
更新权重
```

**梯度消失问题**：
```
长期依赖的梯度呈指数衰减
难以学习长期依赖关系
```

**梯度爆炸问题**：
```
梯度呈指数增长
数值不稳定
```

**解决方法**：
- **梯度裁剪**：限制梯度大小
- **改进架构**：LSTM、GRU
- **正则化**：Dropout、权重衰减

#### 9.1.3 RNN的应用场景

**自然语言处理**：
- **语言建模**：预测下一个词
- **机器翻译**：序列到序列映射
- **文本分类**：情感分析、主题分类
- **命名实体识别**：识别实体

**时间序列预测**：
- **股价预测**：股票价格趋势
- **天气预报**：气象数据预测
- **销售预测**：产品销量预测
- **传感器数据分析**：异常检测

**语音识别**：
- **语音转文本**：自动语音识别
- **说话人识别**：身份验证
- **语音增强**：去噪、增强

**视频处理**：
- **动作识别**：视频中的动作分类
- **视频描述**：生成视频描述
- **视频预测**：预测下一帧

#### 9.1.4 RNN的局限性

**长期依赖问题**：
- 难以学习长期依赖关系
- 梯度消失导致早期信息丢失
- 记忆容量有限

**计算效率**：
- 序列化处理，难以并行
- 长序列训练时间长
- 内存占用大

**模型容量**：
- 简单RNN表达能力有限
- 难以处理复杂模式
- 需要更复杂的架构

**梯度问题**：
- 梯度消失/爆炸
- 训练不稳定
- 需要特殊技巧

### 9.2 LSTM与GRU

#### 9.2.1 LSTM架构

**LSTM的动机**：
解决RNN的长期依赖问题和梯度消失问题。

**LSTM的核心组件**：
```
遗忘门：fₜ = σ(W_f·[hₜ₋₁, xₜ] + b_f)
输入门：iₜ = σ(W_i·[hₜ₋₁, xₜ] + b_i)
候选记忆：Ĉₜ = tanh(W_C·[hₜ₋₁, xₜ] + b_C)
记忆单元：Cₜ = fₜ ⊙ Cₜ₋₁ + iₜ ⊙ Ĉₜ
输出门：oₜ = σ(W_o·[hₜ₋₁, xₜ] + b_o)
隐藏状态：hₜ = oₜ ⊙ tanh(Cₜ)
```

**门控机制的作用**：
- **遗忘门**：决定丢弃什么信息
- **输入门**：决定存储什么新信息
- **输出门**：决定输出什么信息

**LSTM的优势**：
- 可以学习长期依赖
- 缓解梯度消失
- 更好的记忆管理

#### 9.2.2 GRU架构

**GRU的简化**：
GRU是LSTM的简化版本，参数更少。

**GRU的核心组件**：
```
更新门：zₜ = σ(W_z·[hₜ₋₁, xₜ] + b_z)
重置门：rₜ = σ(W_r·[hₜ₋₁, xₜ] + b_r)
候选状态：ĥₜ = tanh(W·[rₜ ⊙ hₜ₋₁, xₜ] + b)
隐藏状态：hₜ = (1 - zₜ) ⊙ hₜ₋₁ + zₜ ⊙ ĥₜ
```

**GRU vs LSTM**：
- **参数更少**：只有两个门
- **计算更快**：简化了运算
- **性能相当**：在很多任务上表现相似
- **更易训练**：收敛速度可能更快

#### 9.2.3 双向RNN

**双向RNN的原理**：
```
前向RNN：→ hₜ→ = f(xₜ, hₜ₋₁→)
后向RNN：← hₜ← = f(xₜ, hₜ₊₁←)
合并：hₜ = [hₜ→, hₜ←] 或 hₜ = g(hₜ→, hₜ←)
```

**双向LSTM/GRU**：
```
结合双向性和门控机制
同时考虑上下文信息
```

**应用场景**：
- **序列标注**：需要完整上下文
- **机器翻译**：编码器使用双向
- **情感分析**：理解完整语义

#### 9.2.4 深层RNN

**深层RNN的挑战**：
- **梯度问题**：更深的网络梯度问题更严重
- **训练困难**：收敛困难，需要技巧
- **过拟合**：模型容量大

**解决方法**：
- **残差连接**：F(x) + x
- **层归一化**：稳定训练
- **Dropout**：防止过拟合
- **跳跃连接**：跳过某些层

**架构选择**：
- **堆叠RNN**：多层RNN堆叠
- **编码器-解码器**：Seq2Seq架构
- **注意力机制**：解决长序列问题

### 9.3 序列建模

#### 9.3.1 序列到序列（Seq2Seq）模型

**Seq2Seq架构**：
```
编码器：输入序列 → 上下文向量
解码器：上下文向量 → 输出序列
```

**编码器-解码器**：
```
编码器：h₁, h₂, ..., hₙ = Encoder(x₁, x₂, ..., xₙ)
上下文：c = hₙ
解码器：y₁, y₂, ..., yₘ = Decoder(c)
```

**训练策略**：
- **Teacher Forcing**：使用真实标签作为输入
- **Scheduled Sampling**：逐步使用预测值
- **Beam Search**：束搜索解码

**应用场景**：
- **机器翻译**：语言转换
- **对话系统**：问答、聊天
- **文本摘要**：长文本到短文本
- **图像描述**：图像到文本

#### 9.3.2 注意力机制

**注意力的动机**：
解决长序列问题和固定长度上下文向量的限制。

**注意力计算**：
```
Query: Q = W_q × hₜ
Key: K = W_k × hᵢ
Value: V = W_v × hᵢ
注意力权重：αₜᵢ = exp(Q·Kᵢ/√d) / Σⱼ exp(Q·Kⱼ/√d)
上下文向量：cₜ = Σᵢ αₜᵢ Vᵢ
```

**注意力类型**：
- **软注意力**：概率分布
- **硬注意力**：单点选择
- **自注意力**：序列内部注意力
- **多头注意力**：并行多个注意力

**注意力的优势**：
- 解决长序列问题
- 提供可解释性
- 更好的性能

#### 9.3.3 Transformer架构

**Transformer的核心思想**：
完全基于注意力机制，摒弃RNN和CNN。

**自注意力机制**：
```
Q = X × W_q, K = X × W_k, V = X × W_v
Attention(Q, K, V) = softmax(QKᵀ/√d)V
```

**多头注意力**：
```
MultiHead(Q, K, V) = Concat(head₁, ..., headₕ)Wᵒ
headᵢ = Attention(QW_qⁱ, KW_kⁱ, VW_vⁱ)
```

**位置编码**：
```
PE(pos, 2i) = sin(pos/10000²ⁱ/d)
PE(pos, 2i+1) = cos(pos/10000²ⁱ/d)
```

**Transformer架构**：
- **编码器**：自注意力 + 前馈网络
- **解码器**：自注意力 + 编码器-解码器注意力 + 前馈网络

#### 9.3.4 预训练语言模型

**语言建模**：
```
自回归语言模型：P(xₜ|x₁, ..., xₜ₋₁)
自编码语言模型：预测被mask的token
```

**BERT（Bidirectional Encoder Representations from Transformers）**：
```
预训练任务：Masked Language Model + Next Sentence Prediction
双向上下文理解
```

**GPT（Generative Pre-trained Transformer）**：
```
自回归预训练
生成能力强
```

**T5（Text-to-Text Transfer Transformer）**：
```
统一为text-to-text格式
多任务预训练
```

### 9.4 RNN的实际应用

#### 9.4.1 文本生成

**语言模型**：
```
基于RNN的语言模型：
P(x₁, x₂, ..., xₙ) = P(x₁)P(x₂|x₁)...P(xₙ|x₁, ..., xₙ₋₁)
```

**文本生成策略**：
- **贪心解码**：选择概率最大的词
- **束搜索**：保持top-k个候选
- **采样**：根据概率分布采样
- **温度采样**：调整采样随机性

**应用场景**：
- **对话系统**：智能客服、聊天机器人
- **创意写作**：诗歌、故事生成
- **代码生成**：自动生成代码
- **营销文案**：广告、产品描述

#### 9.4.2 时间序列预测

**股票预测**：
```
特征工程：价格、成交量、技术指标
模型架构：LSTM + 注意力
预测目标：价格趋势、波动性
```

**能源预测**：
```
电力负荷预测
风能/太阳能预测
设备故障预测
```

**交通预测**：
```
交通流量预测
路径规划
拥堵预测
```

**评估指标**：
- **MAE**：平均绝对误差
- **RMSE**：均方根误差
- **MAPE**：平均绝对百分比误差
- **R²**：决定系数

#### 9.4.3 推荐系统

**序列推荐**：
```
用户行为序列：[item₁, item₂, ..., itemₜ]
预测下一个item：P(itemₜ₊₁|item₁, ..., itemₜ)
```

**会话推荐**：
```
会话内推荐：短期兴趣
跨会话推荐：长期兴趣
```

**模型架构**：
- **RNN-based**：GRU4Rec
- **Attention-based**：SASRec
- **Transformer-based**：BERT4Rec

**评估指标**：
- **Hit Rate**：命中率
- **NDCG**：归一化折损累计增益
- **MRR**：平均倒数排名
- **Coverage**：覆盖率

#### 9.4.4 异常检测

**时间序列异常检测**：
```
基于重构误差的异常检测：
- 训练RNN/LSTM学习正常模式
- 计算重构误差
- 误差大的点为异常点
```

**多变量异常检测**：
```
考虑变量间的关系
使用多维时间序列
```

**应用场景**：
- **金融欺诈**：信用卡欺诈、洗钱
- **网络入侵**：异常访问模式
- **设备故障**：预测性维护
- **医疗监控**：患者状态异常

**异常评分**：
- **重构误差**：自编码器方法
- **预测误差**：预测模型方法
- **密度估计**：概率方法
- **距离度量**：基于距离的方法