# 第五部分：数据工程

## 第13章 数据处理基础

### 13.1 数据清洗

#### 13.1.1 缺失值处理

**缺失值的类型**：
```
完全随机缺失（MCAR）：缺失与其他变量无关
随机缺失（MAR）：缺失与其他观测变量相关
非随机缺失（MNAR）：缺失与缺失值本身相关
```

**缺失值识别**：
```python
import pandas as pd
import numpy as np

# 查看缺失值
df.isnull().sum()
df.isna().sum()

# 缺失值比例
missing_ratio = df.isnull().mean()

# 可视化缺失值
import seaborn as sns
sns.heatmap(df.isnull(), cbar=False)
```

**缺失值处理策略**：
```
删除法：删除缺失值较多的行或列
填充法：用特定值填充缺失值
插值法：基于周围值进行插值
模型法：用模型预测缺失值
```

**常用填充方法**：
```python
# 均值填充
df['column'].fillna(df['column'].mean(), inplace=True)

# 中位数填充
df['column'].fillna(df['column'].median(), inplace=True)

# 众数填充
df['column'].fillna(df['column'].mode()[0], inplace=True)

# 前向填充
df.fillna(method='ffill', inplace=True)

# 后向填充
df.fillna(method='bfill', inplace=True)
```

#### 13.1.2 异常值检测与处理

**异常值的定义**：
明显偏离数据集中其他观测值的数据点，可能是由于测量错误、数据录入错误或其他异常情况造成。

**异常值检测方法**：
```
统计方法：基于统计分布
可视化方法：箱线图、散点图
距离方法：基于距离度量
密度方法：基于局部密度
```

**统计检测方法**：
```python
# Z-score方法
from scipy import stats
z_scores = stats.zscore(df['column'])
outliers = df[np.abs(z_scores) > 3]

# IQR方法
Q1 = df['column'].quantile(0.25)
Q3 = df['column'].quantile(0.75)
IQR = Q3 - Q1
outliers = df[(df['column'] < Q1 - 1.5*IQR) | (df['column'] > Q3 + 1.5*IQR)]
```

**异常值处理策略**：
```
删除：直接删除异常值
替换：用合理值替换
转换：对数转换等
分箱：将值分到不同的箱中
保留：保留作为特殊信息
```

**可视化检测**：
```python
import matplotlib.pyplot as plt

# 箱线图
plt.boxplot(df['column'])

# 直方图
plt.hist(df['column'], bins=50)

# 散点图
plt.scatter(df['x'], df['y'])
```

#### 13.1.3 数据类型转换

**常见数据类型**：
```
数值型：int, float
字符串：str, object
时间日期：datetime
类别型：category
布尔型：bool
```

**类型转换方法**：
```python
# 数值转换
df['column'] = pd.to_numeric(df['column'], errors='coerce')

# 日期转换
df['date_column'] = pd.to_datetime(df['date_column'])

# 类别转换
df['category_column'] = df['category_column'].astype('category')

# 字符串转换
df['column'] = df['column'].astype(str)
```

**特殊类型处理**：
```python
# 处理混合类型
df['mixed_column'] = pd.to_numeric(df['mixed_column'], errors='coerce')

# 处理编码问题
df['text_column'] = df['text_column'].str.encode('utf-8').str.decode('utf-8')

# 处理单位
df['price'] = df['price'].str.replace('$', '').str.replace(',', '').astype(float)
```

**类型推断**：
```python
# 自动类型推断
df = df.infer_objects()

# 转换最佳类型
df = df.convert_dtypes()
```

#### 13.1.4 数据去重与合并

**数据去重**：
```python
# 完全重复
df.drop_duplicates(inplace=True)

# 部分重复
df.drop_duplicates(subset=['column1', 'column2'], inplace=True)

# 保留最后一条
df.drop_duplicates(keep='last')

# 统计重复
df.duplicated().sum()
```

**数据合并**：
```python
# 纵向合并
df_combined = pd.concat([df1, df2], axis=0)

# 横向合并
df_combined = pd.concat([df1, df2], axis=1)

# 内连接
df_merged = pd.merge(df1, df2, on='key', how='inner')

# 左连接
df_merged = pd.merge(df1, df2, on='key', how='left')
```

**数据连接策略**：
```
内连接：只保留匹配的行
左连接：保留左表所有行
右连接：保留右表所有行
外连接：保留所有行
```

**合并后的检查**：
```python
# 检查合并后数据
print(df_combined.shape)
print(df_combined.isnull().sum())

# 验证合并结果
assert len(df_combined) == len(df1) + len(df2) - duplicates
```

### 13.2 特征工程

#### 13.2.1 数值特征处理

**标准化**：
```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df_scaled = scaler.fit_transform(df[['column1', 'column2']])
```

**归一化**：
```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df_normalized = scaler.fit_transform(df[['column1', 'column2']])
```

**分箱处理**：
```python
# 等宽分箱
df['bin'] = pd.cut(df['column'], bins=5)

# 等频分箱
df['bin'] = pd.qcut(df['column'], q=5)

# 自定义分箱
bins = [0, 18, 30, 50, 100]
labels = ['young', 'adult', 'middle', 'senior']
df['age_group'] = pd.cut(df['age'], bins=bins, labels=labels)
```

**数学变换**：
```python
# 对数变换
df['log_column'] = np.log1p(df['column'])

# 平方根变换
df['sqrt_column'] = np.sqrt(df['column'])

# 幂变换
df['power_column'] = np.power(df['column'], 2)

# Box-Cox变换
from scipy import stats
df['boxcox_column'], _ = stats.boxcox(df['column'] + 1)
```

#### 13.2.2 类别特征处理

**标签编码**：
```python
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['encoded_column'] = le.fit_transform(df['category_column'])
```

**独热编码**：
```python
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(sparse=False)
encoded = encoder.fit_transform(df[['category_column']])
```

**目标编码**：
```python
# 目标均值编码
target_mean = df.groupby('category_column')['target'].mean()
df['target_encoded'] = df['category_column'].map(target_mean)
```

**频率编码**：
```python
# 频率编码
frequency = df['category_column'].value_counts(normalize=True)
df['frequency_encoded'] = df['category_column'].map(frequency)
```

**二进制编码**：
```python
import category_encoders as ce

encoder = ce.BinaryEncoder(cols=['category_column'])
df_binary = encoder.fit_transform(df)
```

#### 13.2.3 时间特征处理

**时间组件提取**：
```python
df['year'] = df['date_column'].dt.year
df['month'] = df['date_column'].dt.month
df['day'] = df['date_column'].dt.day
df['hour'] = df['date_column'].dt.hour
df['minute'] = df['date_column'].dt.minute
df['second'] = df['date_column'].dt.second
df['weekday'] = df['date_column'].dt.weekday
df['quarter'] = df['date_column'].dt.quarter
```

**周期性特征**：
```python
# 月份周期性
df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)

# 小时周期性
df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
```

**时间差特征**：
```python
# 时间差计算
df['days_since_start'] = (df['date_column'] - df['date_column'].min()).dt.days
df['weeks_since_start'] = (df['date_column'] - df['date_column'].min()).dt.days / 7
```

**滞后特征**：
```python
# 滞后特征
df['lag_1'] = df['value'].shift(1)
df['lag_7'] = df['value'].shift(7)
df['lag_30'] = df['value'].shift(30)
```

#### 13.2.4 文本特征处理

**词袋模型**：
```python
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(df['text_column'])
```

**TF-IDF特征**：
```python
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer()
X = tfidf.fit_transform(df['text_column'])
```

**N-gram特征**：
```python
# N-gram
tfidf_ngram = TfidfVectorizer(ngram_range=(1, 3))
X = tfidf_ngram.fit_transform(df['text_column'])
```

**文本统计特征**：
```python
df['text_length'] = df['text_column'].str.len()
df['word_count'] = df['text_column'].str.split().str.len()
df['char_count'] = df['text_column'].str.len()
df['avg_word_length'] = df['text_column'].str.split().apply(lambda x: np.mean([len(word) for word in x]))
```

### 13.3 数据变换

#### 13.3.1 数据归一化

**最小-最大归一化**：
```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df_normalized = scaler.fit_transform(df[['column1', 'column2']])
```

**Z-score标准化**：
```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df_standardized = scaler.fit_transform(df[['column1', 'column2']])
```

**鲁棒缩放**：
```python
from sklearn.preprocessing import RobustScaler

scaler = RobustScaler()
df_robust = scaler.fit_transform(df[['column1', 'column2']])
```

**单位向量缩放**：
```python
from sklearn.preprocessing import Normalizer

scaler = Normalizer()
df_normalized = scaler.fit_transform(df[['column1', 'column2']])
```

#### 13.3.2 数据降维

**主成分分析（PCA）**：
```python
from sklearn.decomposition import PCA

pca = PCA(n_components=0.95)  # 保留95%的方差
df_pca = pca.fit_transform(df_scaled)
```

**线性判别分析（LDA）**：
```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

lda = LinearDiscriminantAnalysis(n_components=2)
df_lda = lda.fit_transform(X, y)
```

**t-SNE降维**：
```python
from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, random_state=42)
df_tsne = tsne.fit_transform(df_scaled)
```

**UMAP降维**：
```python
import umap

reducer = umap.UMAP(n_components=2)
df_umap = reducer.fit_transform(df_scaled)
```

#### 13.3.3 特征选择

**过滤法**：
```python
from sklearn.feature_selection import SelectKBest, f_classif

selector = SelectKBest(f_classif, k=10)
X_selected = selector.fit_transform(X, y)
```

**包装法**：
```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

rfe = RFE(estimator=LogisticRegression(), n_features_to_select=10)
X_selected = rfe.fit_transform(X, y)
```

**嵌入法**：
```python
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier

selector = SelectFromModel(RandomForestClassifier(), threshold='median')
X_selected = selector.fit_transform(X, y)
```

**基于树的特征重要性**：
```python
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier()
rf.fit(X, y)
importance = rf.feature_importances_
```

#### 13.3.4 数据增强

**图像数据增强**：
```python
from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)
```

**文本数据增强**：
```python
import nlpaug.augmenter.word as naw

# 同义词替换
aug = naw.SynonymAug(aug_src='wordnet')
augmented_text = aug.augment(text)

# 回译
aug = naw.BackTranslationAug(
    from_model_name='facebook/wmt19-en-de',
    to_model_name='facebook/wmt19-de-en'
)
```

**时序数据增强**：
```python
# 时间扭曲
def time_warp(series, sigma=0.2):
    return np.interp(np.linspace(0, 1, len(series)),
                    np.linspace(0, 1, len(series)) + np.random.normal(0, sigma, len(series)),
                    series)

# 添加噪声
def add_noise(series, noise_level=0.1):
    return series + np.random.normal(0, noise_level * series.std(), len(series))
```

**表格数据增强**：
```python
# SMOTE过采样
from imblearn.over_sampling import SMOTE

smote = SMOTE()
X_resampled, y_resampled = smote.fit_resample(X, y)

# ADASYN过采样
from imblearn.over_sampling import ADASYN

adasyn = ADASYN()
X_resampled, y_resampled = adasyn.fit_resample(X, y)
```

### 13.4 数据质量评估

#### 13.4.1 数据完整性

**完整性指标**：
```python
# 缺失值比例
missing_ratio = df.isnull().mean()

# 完整性得分
completeness_score = 1 - missing_ratio.mean()

# 按列统计完整性
column_completeness = (1 - df.isnull().mean()).to_dict()
```

**完整性报告**：
```python
def completeness_report(df):
    report = {
        'total_rows': len(df),
        'total_columns': len(df.columns),
        'missing_values': df.isnull().sum().sum(),
        'completeness_score': 1 - df.isnull().mean().mean(),
        'column_completeness': (1 - df.isnull().mean()).to_dict()
    }
    return report
```

**完整性可视化**：
```python
import matplotlib.pyplot as plt
import seaborn as sns

# 缺失值热图
plt.figure(figsize=(12, 8))
sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Values Heatmap')
plt.show()

# 缺失值柱状图
missing_ratio = df.isnull().mean()
plt.figure(figsize=(12, 6))
missing_ratio.plot(kind='bar')
plt.title('Missing Values by Column')
plt.ylabel('Missing Ratio')
plt.show()
```

#### 13.4.2 数据准确性

**准确性检查方法**：
```python
# 范围检查
def range_check(df, column, min_val, max_val):
    out_of_range = df[(df[column] < min_val) | (df[column] > max_val)]
    return out_of_range

# 格式检查
def format_check(df, column, pattern):
    import re
    invalid_format = df[~df[column].str.match(pattern, na=False)]
    return invalid_format

# 一致性检查
def consistency_check(df, col1, col2, condition):
    inconsistent = df[~condition(df[col1], df[col2])]
    return inconsistent
```

**业务规则验证**：
```python
# 业务规则示例
def business_rules_validation(df):
    rules = {
        'age_positive': df['age'] >= 0,
        'salary_positive': df['salary'] >= 0,
        'age_reasonable': df['age'] <= 120,
        'email_format': df['email'].str.contains('@'),
        'phone_format': df['phone'].str.match(r'^\+?1?\d{9,15}$')
    }

    violations = {}
    for rule_name, condition in rules.items():
        violations[rule_name] = len(df[~condition])

    return violations
```

**数据质量评分**：
```python
def data_quality_score(df):
    scores = {
        'completeness': 1 - df.isnull().mean().mean(),
        'uniqueness': 1 - df.duplicated().mean(),
        'consistency': 0.9  # 需要基于具体规则计算
    }

    overall_score = np.mean(list(scores.values()))
    return {'overall': overall_score, 'detailed': scores}
```

#### 13.4.3 数据一致性

**一致性类型**：
```
内部一致性：数据内部逻辑一致
跨表一致性：不同表格间数据一致
时间一致性：时间序列数据一致
格式一致性：数据格式统一
```

**一致性检查**：
```python
# 跨表一致性
def cross_table_consistency(df1, df2, key_column):
    merged = pd.merge(df1, df2, on=key_column, how='outer', indicator=True)
    inconsistencies = merged[merged['_merge'] != 'both']
    return inconsistencies

# 时间一致性
def temporal_consistency(df, date_column, value_column):
    # 检查时间序列中的异常跳变
    df_sorted = df.sort_values(date_column)
    changes = df_sorted[value_column].diff().abs()
    outliers = df_sorted[changes > changes.quantile(0.99)]
    return outliers
```

**标准化处理**：
```python
# 统一格式
def standardize_format(df, column, format_func):
    df[column] = df[column].apply(format_func)
    return df

# 统一编码
def standardize_encoding(df, column, target_encoding='utf-8'):
    df[column] = df[column].str.encode(target_encoding, errors='ignore').str.decode(target_encoding)
    return df
```

#### 13.4.4 数据监控

**实时监控**：
```python
class DataMonitor:
    def __init__(self, config):
        self.config = config
        self.alerts = []

    def check_completeness(self, df):
        missing_ratio = df.isnull().mean()
        for col, ratio in missing_ratio.items():
            if ratio > self.config['missing_threshold']:
                self.alerts.append(f"High missing ratio in {col}: {ratio:.2%}")

    def check_freshness(self, df, date_column):
        latest_date = df[date_column].max()
        days_old = (pd.Timestamp.now() - latest_date).days
        if days_old > self.config['freshness_threshold']:
            self.alerts.append(f"Data is {days_old} days old")

    def check_volume(self, df):
        expected_rows = self.config['expected_volume']
        actual_rows = len(df)
        if abs(actual_rows - expected_rows) / expected_rows > self.config['volume_tolerance']:
            self.alerts.append(f"Unexpected data volume: {actual_rows} vs {expected_rows}")
```

**数据质量仪表板**：
```python
import dash
import dash_core_components as dcc
import dash_html_components as html
import plotly.express as px

def create_dashboard(df):
    app = dash.Dash(__name__)

    app.layout = html.Div([
        html.H1("Data Quality Dashboard"),
        dcc.Graph(
            figure=px.imshow(df.isnull(), title="Missing Values")
        ),
        dcc.Graph(
            figure=px.bar(df.dtypes.value_counts(), title="Data Types")
        )
    ])

    return app
```

**报警系统**：
```python
class AlertSystem:
    def __init__(self):
        self.subscribers = []

    def subscribe(self, callback):
        self.subscribers.append(callback)

    def send_alert(self, message, level='warning'):
        alert = {
            'message': message,
            'level': level,
            'timestamp': pd.Timestamp.now()
        }

        for callback in self.subscribers:
            callback(alert)

    def email_alert(self, alert):
        # 发送邮件报警
        pass

    def slack_alert(self, alert):
        # 发送Slack报警
        pass
```

## 第14章 大数据技术

### 14.1 分布式计算基础

#### 14.1.1 分布式系统概念

**分布式系统的定义**：
多台计算机通过网络连接，协同工作以完成共同任务的系统。

**分布式系统的优势**：
```
可扩展性：可以水平扩展
高可用性：单点故障不影响整体
成本效益：使用廉价商用硬件
性能提升：并行处理能力
```

**CAP定理**：
```
一致性（Consistency）：所有节点同时看到相同数据
可用性（Availability）：每个请求都能收到响应
分区容错性（Partition tolerance）：网络分区时系统仍能运行
只能同时满足其中两个特性
```

**分布式系统的挑战**：
```
网络延迟：网络通信的开销
数据一致性：保持数据一致性
故障处理：处理节点故障
负载均衡：均匀分配负载
```

#### 14.1.2 MapReduce编程模型

**MapReduce的基本概念**：
```
Map阶段：将输入数据分解为键值对
Shuffle阶段：按键分组和排序
Reduce阶段：对分组数据进行聚合
```

**MapReduce工作流程**：
```
1. 输入数据分片
2. Map函数处理每个分片
3. Shuffle和Sort阶段
4. Reduce函数处理分组数据
5. 输出结果
```

**Word Count示例**：
```python
# Map函数
def mapper(text):
    words = text.split()
    for word in words:
        yield (word, 1)

# Reduce函数
def reducer(word, counts):
    yield (word, sum(counts))
```

**MapReduce的优化**：
```
Combiner：在Map端进行局部聚合
Partitioner：自定义分区策略
Compression：压缩中间数据
Speculative Execution： speculative执行
```

#### 14.1.3 分布式文件系统

**HDFS架构**：
```
NameNode：管理文件系统元数据
DataNode：存储实际数据块
Secondary NameNode：辅助NameNode
Client：访问HDFS的客户端
```

**HDFS特点**：
```
高容错性：数据块多副本存储
高吞吐量：为批处理优化
大文件存储：适合存储大文件
流式访问：适合一次写入多次读取
```

**HDFS操作**：
```python
from hdfs import InsecureClient

# 连接HDFS
client = InsecureClient('http://namenode:50070', user='hadoop')

# 上传文件
client.upload('/user/hadoop/data.txt', 'local_data.txt')

# 下载文件
client.download('/user/hadoop/data.txt', 'local_data.txt')

# 列出文件
client.list('/user/hadoop')
```

**其他分布式文件系统**：
```
Amazon S3：云存储服务
Google Cloud Storage：谷歌云存储
Azure Blob Storage：微软云存储
Ceph：开源分布式存储
```

#### 14.1.4 分布式数据库

**NoSQL数据库类型**：
```
文档数据库：MongoDB、Couchbase
键值存储：Redis、DynamoDB
列族存储：Cassandra、HBase
图数据库：Neo4j、Amazon Neptune
```

**分区策略**：
```
水平分区：按行分区
垂直分区：按列分区
范围分区：按值范围分区
哈希分区：按哈希值分区
```

**复制策略**：
```
主从复制：单主多从
多主复制：多主多从
环形复制：环形拓扑复制
无主复制：无中心节点
```

**分布式事务**：
```
两阶段提交（2PC）：保证跨节点事务一致性
三阶段提交（3PC）：改进的两阶段提交
SAGA模式：基于补偿的长事务
最终一致性：保证最终达到一致状态
```

### 14.2 Spark基础

#### 14.2.1 Spark架构与组件

**Spark核心架构**：
```
Driver Program：运行用户main函数
Cluster Manager：资源管理（YARN、Mesos）
Executors：运行任务的工作节点
SparkContext：Spark程序的入口点
```

**Spark组件**：
```
Spark Core：核心功能
Spark SQL：SQL和结构化数据处理
Spark Streaming：流处理
MLlib：机器学习库
GraphX：图处理库
```

**Spark运行模式**：
```
Local Mode：本地模式
Standalone Mode：独立模式
YARN Mode：运行在YARN上
Mesos Mode：运行在Mesos上
```

**Spark应用结构**：
```python
from pyspark.sql import SparkSession

# 创建SparkSession
spark = SparkSession.builder \
    .appName("MyApp") \
    .master("local[*]") \
    .getOrCreate()

# 读取数据
df = spark.read.csv("data.csv", header=True)

# 处理数据
result = df.groupBy("category").count()

# 显示结果
result.show()

# 停止SparkSession
spark.stop()
```

#### 14.2.2 RDD编程

**RDD基本概念**：
```
弹性分布式数据集（RDD）
不可变的分布式对象集合
可以通过两种方式创建：并行化集合、引用外部存储系统
支持两种操作：转换（transformation）和动作（action）
```

**RDD创建**：
```python
# 并行化集合
rdd = sc.parallelize([1, 2, 3, 4, 5])

# 从外部数据源创建
rdd = sc.textFile("hdfs://path/to/file.txt")

# 从DataFrame创建
rdd = df.rdd
```

**RDD转换操作**：
```python
# map操作
rdd_map = rdd.map(lambda x: x * 2)

# filter操作
rdd_filter = rdd.filter(lambda x: x > 3)

# flatMap操作
rdd_flatmap = rdd.flatMap(lambda x: [x, x+1])

# reduceByKey操作
rdd_pairs = sc.parallelize([('a', 1), ('b', 2), ('a', 3)])
rdd_reduce = rdd_pairs.reduceByKey(lambda x, y: x + y)
```

**RDD动作操作**：
```python
# collect操作
data = rdd.collect()

# count操作
count = rdd.count()

# reduce操作
sum_result = rdd.reduce(lambda x, y: x + y)

# take操作
first_5 = rdd.take(5)
```

#### 14.2.3 DataFrame与Dataset API

**DataFrame vs RDD**：
```
DataFrame：带Schema的分布式数据集
RDD：无Schema的弹性分布式数据集
DataFrame：执行计划优化
RDD：更灵活，但优化较少
```

**DataFrame操作**：
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum, avg

# 创建DataFrame
data = [("Alice", 34), ("Bob", 45), ("Charlie", 29)]
df = spark.createDataFrame(data, ["name", "age"])

# 查询操作
df.select("name", "age").show()
df.filter(df.age > 30).show()
df.groupBy("age").count().show()

# SQL查询
df.createOrReplaceTempView("people")
result = spark.sql("SELECT * FROM people WHERE age > 30")
```

**DataFrame高级操作**：
```python
# 窗口函数
from pyspark.sql import Window
from pyspark.sql.functions import row_number

window_spec = Window.partitionBy("department").orderBy("salary")
df.withColumn("rank", row_number().over(window_spec))

# 用户定义函数（UDF）
from pyspark.sql.functions import udf
from pyspark.sql.types import IntegerType

def age_category(age):
    if age < 30: return "Young"
    elif age < 50: return "Middle"
    else: return "Senior"

age_udf = udf(age_category, StringType())
df.withColumn("age_category", age_udf(df.age))
```

**数据源操作**：
```python
# 读取CSV
df = spark.read.csv("data.csv", header=True, inferSchema=True)

# 读取JSON
df = spark.read.json("data.json")

# 读取Parquet
df = spark.read.parquet("data.parquet")

# 写入数据
df.write.parquet("output.parquet")
df.write.json("output.json")
```

#### 14.2.4 Spark SQL

**Spark SQL特点**：
```
统一的DataFrame/Dataset API
SQL和DataFrame/Dataset的互操作
Catalyst优化器
支持多种数据源
```

**SQL查询**：
```python
# 创建临时视图
df.createOrReplaceTempView("employees")

# 执行SQL查询
result = spark.sql("""
    SELECT department,
           AVG(salary) as avg_salary,
           COUNT(*) as employee_count
    FROM employees
    GROUP BY department
    ORDER BY avg_salary DESC
""")

# 复杂查询
complex_query = spark.sql("""
    SELECT e1.name, e1.salary, e1.department,
           (SELECT AVG(e2.salary) FROM employees e2
            WHERE e2.department = e1.department) as dept_avg
    FROM employees e1
    WHERE e1.salary > (SELECT AVG(e3.salary) FROM employees e3)
""")
```

**性能优化**：
```python
# 缓存表
df.cache()

# 分区优化
df.repartition(10, "department")

# 广播连接
from pyspark.sql.functions import broadcast

large_df.join(broadcast(small_df), "key")

# 查询计划分析
df.explain()
```

**数据类型和函数**：
```python
# 数据类型
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

schema = StructType([
    StructField("name", StringType(), True),
    StructField("age", IntegerType(), True)
])

# 内置函数
from pyspark.sql.functions import col, concat, lit

df.withColumn("full_name", concat(col("first_name"), lit(" "), col("last_name")))
```

### 14.3 数据管道

#### 14.3.1 ETL基础

**ETL流程**：
```
Extract（提取）：从数据源提取数据
Transform（转换）：清洗和转换数据
Load（加载）：将数据加载到目标系统
```

**ETL工具**：
```
Apache Airflow：工作流调度
Apache NiFi：数据流管理
Talend：数据集成平台
Informatica：企业数据集成
```

**ETL设计原则**：
```
模块化：将复杂流程分解为模块
可重用性：组件可以重复使用
可扩展性：支持数据量增长
容错性：处理异常和错误
```

**数据提取策略**：
```python
# 批量提取
def batch_extract(source_system, last_extract_time):
    query = f"SELECT * FROM table WHERE updated_at > '{last_extract_time}'"
    data = source_system.execute_query(query)
    return data

# 增量提取
def incremental_extract(source_system, watermark):
    new_data = source_system.get_changes_since(watermark)
    return new_data

# 实时提取
def real_time_extract(stream_source):
    for record in stream_source:
        yield record
```

#### 14.3.2 数据流处理

**流处理框架**：
```
Apache Kafka：分布式消息队列
Apache Flink：流处理框架
Apache Storm：实时计算系统
Spark Streaming：微批处理
```

**Kafka基础**：
```python
from kafka import KafkaProducer, KafkaConsumer

# 生产者
producer = KafkaProducer(bootstrap_servers='localhost:9092')
producer.send('topic_name', value=b'Hello, Kafka!')

# 消费者
consumer = KafkaConsumer('topic_name',
                        bootstrap_servers='localhost:9092')
for message in consumer:
    print(message.value)
```

**Spark Streaming**：
```python
from pyspark.streaming import StreamingContext

# 创建StreamingContext
ssc = StreamingContext(sc, batchDuration=10)

# 创建DStream
lines = ssc.socketTextStream("localhost", 9999)

# 处理数据流
words = lines.flatMap(lambda line: line.split(" "))
pairs = words.map(lambda word: (word, 1))
word_counts = pairs.reduceByKey(lambda a, b: a + b)

# 输出结果
word_counts.pprint()

# 启动流处理
ssc.start()
ssc.awaitTermination()
```

**流处理模式**：
```
微批处理：将流分割为小批量处理
事件时间处理：基于事件发生时间处理
状态管理：维护流处理状态
窗口操作：时间窗口聚合
```

#### 14.3.3 数据质量检查

**数据质量维度**：
```
完整性：数据是否完整
准确性：数据是否准确
一致性：数据是否一致
及时性：数据是否及时
唯一性：数据是否唯一
```

**数据质量规则**：
```python
class DataQualityRules:
    def __init__(self):
        self.rules = []

    def add_rule(self, name, rule_func, severity='error'):
        self.rules.append({
            'name': name,
            'function': rule_func,
            'severity': severity
        })

    def validate(self, data):
        results = []
        for rule in self.rules:
            try:
                result = rule['function'](data)
                results.append({
                    'rule_name': rule['name'],
                    'result': result,
                    'severity': rule['severity']
                })
            except Exception as e:
                results.append({
                    'rule_name': rule['name'],
                    'result': False,
                    'error': str(e),
                    'severity': 'error'
                })
        return results
```

**质量检查示例**：
```python
# 定义质量规则
def completeness_check(df):
    return df.isnull().sum().sum() == 0

def range_check(df, column, min_val, max_val):
    return df[column].between(min_val, max_val).all()

def uniqueness_check(df, column):
    return df[column].nunique() == len(df)

# 执行检查
dq = DataQualityRules()
dq.add_rule('completeness', completeness_check)
dq.add_rule('age_range', lambda df: range_check(df, 'age', 0, 120))
dq.add_rule('email_unique', lambda df: uniqueness_check(df, 'email'))

results = dq.validate(df)
```

#### 14.3.4 管道监控与优化

**性能监控**：
```python
class PipelineMonitor:
    def __init__(self):
        self.metrics = {}

    def record_metric(self, name, value, timestamp=None):
        if timestamp is None:
            timestamp = pd.Timestamp.now()

        if name not in self.metrics:
            self.metrics[name] = []

        self.metrics[name].append({
            'timestamp': timestamp,
            'value': value
        })

    def get_metrics(self, name, start_time=None, end_time=None):
        if name not in self.metrics:
            return []

        metrics = self.metrics[name]

        if start_time:
            metrics = [m for m in metrics if m['timestamp'] >= start_time]
        if end_time:
            metrics = [m for m in metrics if m['timestamp'] <= end_time]

        return metrics

    def get_statistics(self, name):
        metrics = self.get_metrics(name)
        values = [m['value'] for m in metrics]

        return {
            'count': len(values),
            'mean': np.mean(values),
            'median': np.median(values),
            'std': np.std(values),
            'min': min(values),
            'max': max(values)
        }
```

**错误处理**：
```python
class PipelineErrorHandler:
    def __init__(self):
        self.error_log = []

    def handle_error(self, error, context):
        error_info = {
            'timestamp': pd.Timestamp.now(),
            'error_type': type(error).__name__,
            'error_message': str(error),
            'context': context,
            'severity': self._determine_severity(error)
        }

        self.error_log.append(error_info)
        self._take_action(error_info)

    def _determine_severity(self, error):
        if isinstance(error, (ValueError, TypeError)):
            return 'warning'
        elif isinstance(error, (ConnectionError, TimeoutError)):
            return 'error'
        else:
            return 'critical'

    def _take_action(self, error_info):
        if error_info['severity'] == 'warning':
            self._log_warning(error_info)
        elif error_info['severity'] == 'error':
            self._log_error(error_info)
            self._retry_operation(error_info)
        else:
            self._log_critical(error_info)
            self._alert_team(error_info)
```

**优化策略**：
```python
class PipelineOptimizer:
    def __init__(self, pipeline):
        self.pipeline = pipeline

    def optimize_parallelism(self):
        # 基于数据大小调整并行度
        data_size = self.pipeline.estimate_data_size()
        optimal_parallelism = self._calculate_optimal_parallelism(data_size)
        self.pipeline.set_parallelism(optimal_parallelism)

    def optimize_memory(self):
        # 优化内存使用
        self.pipeline.enable_memory_optimization()

    def optimize_io(self):
        # 优化IO操作
        self.pipeline.use_columnar_format()
        self.pipeline.enable_compression()

    def _calculate_optimal_parallelism(self, data_size):
        # 计算最优并行度
        cores = psutil.cpu_count()
        memory_gb = psutil.virtual_memory().total / (1024**3)

        # 基于数据量和可用资源计算
        return min(cores * 2, int(data_size / (1024 * 1024 * 100)))  # 每100MB数据一个分区
```

### 14.4 实时数据处理

#### 14.4.1 流处理概念

**流处理的定义**：
实时处理连续不断产生的数据流，低延迟地处理和分析数据。

**流处理 vs 批处理**：
```
流处理：实时处理，低延迟，有状态
批处理：批量处理，高延迟，无状态
流处理：适合实时分析、监控
批处理：适合历史分析、报告
```

**流处理架构**：
```
数据源：产生实时数据
消息队列：缓冲和传输数据
流处理引擎：处理数据流
存储系统：存储处理结果
可视化：展示实时结果
```

**流处理的挑战**：
```
低延迟：需要快速处理
高吞吐：处理大量数据
容错性：处理故障和恢复
状态管理：维护处理状态
```

#### 14.4.2 Kafka消息队列

**Kafka架构**：
```
Producer：消息生产者
Consumer：消息消费者
Broker：消息代理服务器
Topic：消息主题
Partition：主题分区
```

**Kafka核心概念**：
```
Topic：消息的逻辑分类
Partition：Topic的物理分区
Offset：消息在分区中的位置
Consumer Group：消费者组
Replication：数据复制
```

**Kafka操作**：
```python
from kafka import KafkaAdminClient, KafkaProducer, KafkaConsumer
from kafka.admin import NewTopic

# 创建主题
admin_client = KafkaAdminClient(bootstrap_servers='localhost:9092')
topic = NewTopic(name='my_topic', num_partitions=3, replication_factor=2)
admin_client.create_topics([topic])

# 生产者配置
producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

# 发送消息
producer.send('my_topic', {'key': 'value'})
producer.flush()

# 消费者配置
consumer = KafkaConsumer(
    'my_topic',
    bootstrap_servers='localhost:9092',
    value_deserializer=lambda m: json.loads(m.decode('utf-8')),
    group_id='my_group'
)

# 消费消息
for message in consumer:
    print(f"Received: {message.value}")
```

**Kafka高级特性**：
```
事务：确保消息处理的原子性
幂等性：防止消息重复处理
流处理：Kafka Streams API
连接器：Kafka Connect
```

#### 14.4.3 Flink流处理

**Flink特点**：
```
真正的流处理：事件时间处理
精确一次：Exactly-once语义
状态管理：有状态计算
窗口操作：灵活的窗口操作
```

**Flink程序结构**：
```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment

# 创建执行环境
env = StreamExecutionEnvironment.get_execution_environment()
t_env = StreamTableEnvironment.create(stream_execution_environment=env)

# 创建数据流
data_stream = env.from_collection([
    (1, 'Alice'),
    (2, 'Bob'),
    (3, 'Charlie')
])

# 处理数据流
result = data_stream.map(lambda x: (x[0], x[1].upper()))

# 输出结果
result.print()

# 执行程序
env.execute("Flink Job")
```

**Flink窗口操作**：
```python
from pyflink.datastream import TimeCharacteristic, WindowedStream

# 设置时间特性
env.set_stream_time_characteristic(TimeCharacteristic.EventTime)

# 滚动窗口
windowed_stream = data_stream \
    .key_by(lambda x: x[0]) \
    .time_window(Time.seconds(10))

# 滑动窗口
sliding_window = data_stream \
    .key_by(lambda x: x[0]) \
    .time_window(Time.seconds(30), Time.seconds(10))

# 会话窗口
session_window = data_stream \
    .key_by(lambda x: x[0]) \
    .window(EventTimeSessionWindows.with_gap(Time.seconds(15)))
```

**Flink状态管理**：
```python
from pyflink.datastream.state import ValueStateDescriptor

class CountFunction(KeyedProcessFunction):
    def __init__(self):
        self.state_desc = ValueStateDescriptor("count", Types.INT())

    def open(self, runtime_context):
        self.count_state = runtime_context.get_state(self.state_desc)

    def process_element(self, value, ctx):
        count = self.count_state.value() or 0
        count += 1
        self.count_state.update(count)
        yield (value[0], count)
```

#### 14.4.4 实时应用场景

**实时监控**：
```python
class RealTimeMonitor:
    def __init__(self, alert_threshold=100):
        self.alert_threshold = alert_threshold
        self.alert_system = AlertSystem()

    def process_metric(self, metric):
        # 检查是否超过阈值
        if metric['value'] > self.alert_threshold:
            self.alert_system.send_alert(
                f"High value detected: {metric['value']}",
                level='warning'
            )

        # 计算滑动平均
        return self._calculate_moving_average(metric)

    def _calculate_moving_average(self, metric, window_size=10):
        # 实现滑动平均计算
        pass
```

**实时推荐**：
```python
class RealTimeRecommender:
    def __init__(self, user_model, item_model):
        self.user_model = user_model
        self.item_model = item_model
        self.user_profiles = {}

    def update_user_profile(self, user_id, item_id, rating):
        # 实时更新用户画像
        if user_id not in self.user_profiles:
            self.user_profiles[user_id] = {'items': [], 'ratings': []}

        self.user_profiles[user_id]['items'].append(item_id)
        self.user_profiles[user_id]['ratings'].append(rating)

    def recommend(self, user_id, top_k=10):
        # 实时推荐
        user_profile = self.user_profiles.get(user_id, {})
        recommendations = self._generate_recommendations(user_profile)
        return recommendations[:top_k]
```

**实时异常检测**：
```python
class RealTimeAnomalyDetector:
    def __init__(self, model):
        self.model = model
        self.normal_patterns = {}

    def detect_anomaly(self, data_point):
        # 实时异常检测
        features = self._extract_features(data_point)

        # 使用预训练模型检测
        is_anomaly = self.model.predict([features])[0]

        if is_anomaly:
            self._trigger_alert(data_point, features)

        return is_anomaly

    def update_normal_patterns(self, data_stream):
        # 实时更新正常模式
        for data_point in data_stream:
            features = self._extract_features(data_point)
            self._update_patterns(features)
```

## 第15章 数据质量管理

### 15.1 数据验证

#### 15.1.1 数据验证框架

**数据验证的重要性**：
确保数据的准确性、完整性和一致性，为下游应用提供高质量数据。

**验证框架组件**：
```
验证规则定义：定义验证规则和约束
验证执行引擎：执行验证规则
验证结果报告：生成验证报告
异常处理：处理验证失败的情况
```

**验证类型**：
```
结构验证：验证数据结构和格式
业务验证：验证业务规则和约束
完整性验证：验证数据完整性
一致性验证：验证数据一致性
```

**验证框架设计**：
```python
class DataValidationFramework:
    def __init__(self):
        self.validators = {}
        self.validation_results = []

    def register_validator(self, name, validator_func):
        """注册验证器"""
        self.validators[name] = validator_func

    def validate_data(self, data, rules):
        """验证数据"""
        results = []
        for rule in rules:
            validator = self.validators.get(rule['type'])
            if validator:
                result = validator(data, rule)
                results.append(result)
        return results

    def generate_report(self, results):
        """生成验证报告"""
        report = {
            'total_rules': len(results),
            'passed_rules': len([r for r in results if r['passed']]),
            'failed_rules': len([r for r in results if not r['passed']]),
            'details': results
        }
        return report
```

#### 15.1.2 结构验证

**结构验证内容**：
```
数据类型验证：验证字段数据类型
数据格式验证：验证数据格式
数据范围验证：验证数据值范围
数据长度验证：验证数据长度
```

**结构验证规则**：
```python
def validate_data_type(data, rule):
    """验证数据类型"""
    field_name = rule['field']
    expected_type = rule['type']

    if field_name not in data:
        return {
            'rule': rule,
            'passed': False,
            'message': f"Field {field_name} not found"
        }

    actual_type = type(data[field_name]).__name__
    if actual_type != expected_type:
        return {
            'rule': rule,
            'passed': False,
            'message': f"Expected {expected_type}, got {actual_type}"
        }

    return {'rule': rule, 'passed': True}

def validate_format(data, rule):
    """验证数据格式"""
    field_name = rule['field']
    pattern = rule['pattern']

    if field_name not in data:
        return {
            'rule': rule,
            'passed': False,
            'message': f"Field {field_name} not found"
        }

    import re
    if not re.match(pattern, str(data[field_name])):
        return {
            'rule': rule,
            'passed': False,
            'message': f"Format validation failed for {field_name}"
        }

    return {'rule': rule, 'passed': True}
```

**数据范围验证**：
```python
def validate_range(data, rule):
    """验证数据范围"""
    field_name = rule['field']
    min_val = rule.get('min')
    max_val = rule.get('max')

    if field_name not in data:
        return {
            'rule': rule,
            'passed': False,
            'message': f"Field {field_name} not found"
        }

    value = data[field_name]

    if min_val is not None and value < min_val:
        return {
            'rule': rule,
            'passed': False,
            'message': f"Value {value} is less than minimum {min_val}"
        }

    if max_val is not None and value > max_val:
        return {
            'rule': rule,
            'passed': False,
            'message': f"Value {value} is greater than maximum {max_val}"
        }

    return {'rule': rule, 'passed': True}
```

#### 15.1.3 业务验证

**业务验证定义**：
基于业务规则和约束验证数据，确保数据符合业务逻辑和需求。

**业务验证类型**：
```
唯一性验证：验证数据唯一性
关联性验证：验证数据间的关联关系
业务规则验证：验证业务规则
时间序列验证：验证时间序列数据
```

**唯一性验证**：
```python
def validate_uniqueness(data, rule):
    """验证数据唯一性"""
    field_name = rule['field']

    if field_name not in data:
        return {
            'rule': rule,
            'passed': False,
            'message': f"Field {field_name} not found"
        }

    values = data[field_name]
    unique_values = set(values)

    if len(values) != len(unique_values):
        duplicates = len(values) - len(unique_values)
        return {
            'rule': rule,
            'passed': False,
            'message': f"Found {duplicates} duplicate values in {field_name}"
        }

    return {'rule': rule, 'passed': True}
```

**关联性验证**：
```python
def validate_relationship(data, rule):
    """验证数据关联关系"""
    field1 = rule['field1']
    field2 = rule['field2']
    relationship = rule['relationship']

    if field1 not in data or field2 not in data:
        return {
            'rule': rule,
            'passed': False,
            'message': "Fields not found"
        }

    if relationship == 'one_to_many':
        # 验证一对多关系
        pass
    elif relationship == 'many_to_many':
        # 验证多对多关系
        pass

    return {'rule': rule, 'passed': True}
```

#### 15.1.4 验证自动化

**自动化验证流程**：
```
1. 定义验证规则
2. 自动执行验证
3. 生成验证报告
4. 自动处理异常
5. 发送报警通知
```

**验证自动化实现**：
```python
class AutomatedDataValidator:
    def __init__(self, config):
        self.config = config
        self.validation_engine = DataValidationFramework()
        self.alert_system = AlertSystem()

        # 注册验证器
        self._register_validators()

    def _register_validators(self):
        """注册验证器"""
        self.validation_engine.register_validator('data_type', validate_data_type)
        self.validation_engine.register_validator('format', validate_format)
        self.validation_engine.register_validator('range', validate_range)
        self.validation_engine.register_validator('uniqueness', validate_uniqueness)

    def validate_dataset(self, dataset, rules):
        """自动验证数据集"""
        try:
            results = self.validation_engine.validate_data(dataset, rules)
            report = self.validation_engine.generate_report(results)

            # 处理验证结果
            self._handle_validation_results(report)

            return report

        except Exception as e:
            self.alert_system.send_alert(f"Validation failed: {str(e)}", 'error')
            raise

    def _handle_validation_results(self, report):
        """处理验证结果"""
        if report['failed_rules'] > 0:
            # 发送报警
            self.alert_system.send_alert(
                f"Data validation failed: {report['failed_rules']} rules failed",
                'warning'
            )

            # 记录详细错误
            for result in report['details']:
                if not result['passed']:
                    self.alert_system.send_alert(
                        f"Rule failed: {result['rule']['name']} - {result['message']}",
                        'info'
                    )
```

**持续验证**：
```python
class ContinuousValidator:
    def __init__(self, data_source, validation_interval=3600):
        self.data_source = data_source
        self.validation_interval = validation_interval
        self.validator = AutomatedDataValidator({})
        self.running = False

    def start_validation(self):
        """启动持续验证"""
        self.running = True

        while self.running:
            try:
                # 获取最新数据
                data = self.data_source.get_latest_data()

                # 执行验证
                report = self.validator.validate_dataset(data, self.get_validation_rules())

                # 更新验证状态
                self.update_validation_status(report)

                # 等待下次验证
                time.sleep(self.validation_interval)

            except Exception as e:
                self.handle_validation_error(e)

    def stop_validation(self):
        """停止持续验证"""
        self.running = False
```

### 15.2 数据监控

#### 15.2.1 监控指标体系

**数据质量指标**：
```
完整性指标：缺失值比例、覆盖率
准确性指标：错误率、准确率
一致性指标：一致性违反次数
及时性指标：数据延迟、更新频率
唯一性指标：重复率、唯一性比例
```

**数据量指标**：
```
数据总量：数据集大小
数据增长率：数据增长速度
数据分布：数据分布情况
数据新鲜度：数据更新时间
```

**数据使用指标**：
```
查询频率：数据查询次数
查询响应时间：查询性能
访问模式：数据访问模式
用户满意度：用户反馈
```

**指标计算**：
```python
class DataQualityMetrics:
    def __init__(self):
        self.metrics = {}

    def calculate_completeness(self, data):
        """计算完整性指标"""
        total_cells = len(data) * len(data.columns)
        missing_cells = data.isnull().sum().sum()
        completeness = 1 - (missing_cells / total_cells)

        return {
            'completeness_ratio': completeness,
            'missing_cells': missing_cells,
            'total_cells': total_cells
        }

    def calculate_accuracy(self, data, ground_truth):
        """计算准确性指标"""
        # 假设有真实值数据
        correct_predictions = (data == ground_truth).sum().sum()
        total_predictions = len(data) * len(data.columns)
        accuracy = correct_predictions / total_predictions

        return {
            'accuracy_ratio': accuracy,
            'correct_predictions': correct_predictions,
            'total_predictions': total_predictions
        }

    def calculate_timeliness(self, data, timestamp_column):
        """计算及时性指标"""
        current_time = pd.Timestamp.now()
        data_timestamps = pd.to_datetime(data[timestamp_column])

        max_delay = (current_time - data_timestamps).max()
        avg_delay = (current_time - data_timestamps).mean()

        return {
            'max_delay_hours': max_delay.total_seconds() / 3600,
            'avg_delay_hours': avg_delay.total_seconds() / 3600,
            'freshest_data': data_timestamps.max(),
            'stalest_data': data_timestamps.min()
        }
```

#### 15.2.2 实时监控

**实时监控系统**：
```
数据采集：实时收集监控数据
数据处理：处理和分析监控数据
报警机制：异常情况报警
可视化展示：实时展示监控状态
```

**实时监控实现**：
```python
class RealTimeDataMonitor:
    def __init__(self, config):
        self.config = config
        self.metrics_collector = MetricsCollector()
        self.alert_system = AlertSystem()
        self.dashboard = MonitoringDashboard()

        # 启动监控任务
        self.start_monitoring()

    def start_monitoring(self):
        """启动实时监控"""
        while True:
            try:
                # 收集指标
                metrics = self.collect_metrics()

                # 分析指标
                analysis = self.analyze_metrics(metrics)

                # 检查异常
                self.check_anomalies(analysis)

                # 更新仪表板
                self.dashboard.update(analysis)

                # 等待下次检查
                time.sleep(self.config['monitoring_interval'])

            except Exception as e:
                self.handle_monitoring_error(e)

    def collect_metrics(self):
        """收集监控指标"""
        metrics = {}

        # 数据量指标
        metrics['data_volume'] = self.metrics_collector.collect_data_volume()

        # 数据质量指标
        metrics['data_quality'] = self.metrics_collector.collect_quality_metrics()

        # 系统性能指标
        metrics['system_performance'] = self.metrics_collector.collect_system_metrics()

        return metrics

    def analyze_metrics(self, metrics):
        """分析指标数据"""
        analysis = {
            'trends': self.calculate_trends(metrics),
            'anomalies': self.detect_anomalies(metrics),
            'predictions': self.predict_future_values(metrics)
        }
        return analysis
```

**异常检测**：
```python
class AnomalyDetector:
    def __init__(self, threshold=3.0):
        self.threshold = threshold
        self.historical_data = []

    def detect_statistical_anomaly(self, value, history):
        """统计异常检测"""
        if len(history) < 10:
            return False

        mean = np.mean(history)
        std = np.std(history)

        z_score = (value - mean) / std if std > 0 else 0

        return abs(z_score) > self.threshold

    def detect_trend_anomaly(self, time_series):
        """趋势异常检测"""
        if len(time_series) < 5:
            return False

        # 计算趋势
        x = np.arange(len(time_series))
        slope, _ = np.polyfit(x, time_series, 1)

        # 检查趋势是否异常
        return abs(slope) > self.threshold

    def detect_pattern_anomaly(self, pattern, normal_pattern):
        """模式异常检测"""
        # 使用距离度量或模式匹配
        distance = np.linalg.norm(pattern - normal_pattern)
        return distance > self.threshold
```

#### 15.2.3 可视化监控

**监控仪表板**：
```
实时指标展示：实时显示关键指标
历史趋势图：显示历史趋势
异常报警：异常情况突出显示
详细信息：点击查看详细信息
```

**可视化实现**：
```python
import plotly.graph_objects as go
from plotly.subplots import make_subplots

class MonitoringDashboard:
    def __init__(self):
        self.figures = {}
        self.create_dashboard()

    def create_dashboard(self):
        """创建监控仪表板"""
        # 创建子图
        self.figures['main'] = make_subplots(
            rows=2, cols=2,
            subplot_titles=('Data Quality', 'Data Volume',
                          'System Performance', 'Anomaly Detection')
        )

        # 数据质量图
        self.figures['quality'] = go.Figure()
        self.figures['quality'].add_trace(go.Scatter(
            x=[], y=[], mode='lines+markers',
            name='Data Quality Score'
        ))

        # 数据量图
        self.figures['volume'] = go.Figure()
        self.figures['volume'].add_trace(go.Bar(
            x=[], y=[], name='Data Volume'
        ))

    def update_dashboard(self, metrics):
        """更新仪表板"""
        # 更新数据质量图
        self.update_quality_chart(metrics['quality'])

        # 更新数据量图
        self.update_volume_chart(metrics['volume'])

        # 更新系统性能图
        self.update_performance_chart(metrics['performance'])

    def update_quality_chart(self, quality_metrics):
        """更新数据质量图"""
        self.figures['quality'].add_trace(go.Scatter(
            x=pd.Timestamp.now(),
            y=quality_metrics['overall_score'],
            mode='lines+markers',
            name='Quality Score'
        ))

    def render_dashboard(self):
        """渲染仪表板"""
        # 使用Dash或Flask创建Web界面
        pass
```

**实时图表**：
```python
class RealTimeChart:
    def __init__(self, max_points=100):
        self.max_points = max_points
        self.data = {'x': [], 'y': []}
        self.figure = go.Figure()

        # 设置图表
        self.figure.update_layout(
            title='Real-time Data Quality',
            xaxis_title='Time',
            yaxis_title='Quality Score',
            showlegend=True
        )

    def add_data_point(self, timestamp, value):
        """添加数据点"""
        self.data['x'].append(timestamp)
        self.data['y'].append(value)

        # 保持数据点数量在限制内
        if len(self.data['x']) > self.max_points:
            self.data['x'].pop(0)
            self.data['y'].pop(0)

        # 更新图表
        self.update_chart()

    def update_chart(self):
        """更新图表"""
        self.figure.data[0].x = self.data['x']
        self.figure.data[0].y = self.data['y']

    def get_figure(self):
        """获取图表对象"""
        return self.figure
```

#### 15.2.4 监控报警

**报警系统设计**：
```
报警规则：定义报警条件
报警级别：不同级别的报警
报警方式：邮件、短信、Slack等
报警抑制：避免重复报警
```

**报警实现**：
```python
class AlertSystem:
    def __init__(self, config):
        self.config = config
        self.alert_history = []
        self.alert_suppression = {}

        # 报警方式
        self.notifiers = {
            'email': EmailNotifier(config['email']),
            'slack': SlackNotifier(config['slack']),
            'sms': SMSNotifier(config['sms'])
        }

    def send_alert(self, message, level='warning', source='system'):
        """发送报警"""
        alert_id = self._generate_alert_id(message, source)

        # 检查是否需要抑制报警
        if self._should_suppress_alert(alert_id, level):
            return

        # 发送报警
        alert = {
            'id': alert_id,
            'message': message,
            'level': level,
            'source': source,
            'timestamp': pd.Timestamp.now()
        }

        self._deliver_alert(alert)
        self._record_alert(alert)

    def _deliver_alert(self, alert):
        """发送报警通知"""
        # 根据级别选择通知方式
        if alert['level'] == 'critical':
            self._send_critical_alert(alert)
        elif alert['level'] == 'warning':
            self._send_warning_alert(alert)
        else:
            self._send_info_alert(alert)

    def _send_critical_alert(self, alert):
        """发送严重报警"""
        # 邮件
        self.notifiers['email'].send_alert(alert)
        # Slack
        self.notifiers['slack'].send_alert(alert)
        # 短信
        self.notifiers['sms'].send_alert(alert)

    def _should_suppress_alert(self, alert_id, level):
        """检查是否需要抑制报警"""
        if alert_id in self.alert_suppression:
            last_alert_time = self.alert_suppression[alert_id]
            suppression_period = self.config.get('suppression_period', 300)  # 5分钟

            if (pd.Timestamp.now() - last_alert_time).total_seconds() < suppression_period:
                return True

        self.alert_suppression[alert_id] = pd.Timestamp.now()
        return False
```

**报警规则引擎**：
```python
class AlertRuleEngine:
    def __init__(self):
        self.rules = []

    def add_rule(self, rule):
        """添加报警规则"""
        self.rules.append(rule)

    def evaluate_rules(self, metrics):
        """评估报警规则"""
        triggered_alerts = []

        for rule in self.rules:
            if rule.evaluate(metrics):
                alert = {
                    'rule_name': rule.name,
                    'message': rule.message,
                    'level': rule.level,
                    'metrics': metrics
                }
                triggered_alerts.append(alert)

        return triggered_alerts

    def add_threshold_rule(self, name, metric, threshold, operator='>', level='warning'):
        """添加阈值规则"""
        rule = ThresholdRule(name, metric, threshold, operator, level)
        self.add_rule(rule)

    def add_trend_rule(self, name, metric, trend_type, level='warning'):
        """添加趋势规则"""
        rule = TrendRule(name, metric, trend_type, level)
        self.add_rule(rule)
```

### 15.3 数据治理

#### 15.3.1 数据治理框架

**数据治理定义**：
数据治理是对数据资产的全面管理，包括数据质量、数据安全、数据生命周期等方面。

**治理框架组件**：
```
数据治理委员会：制定治理策略
数据质量团队：负责数据质量管理
数据安全团队：负责数据安全
数据架构团队：负责数据架构设计
```

**治理原则**：
```
数据驱动：以数据价值为导向
标准化：统一数据标准
合规性：符合法规要求
持续性：持续改进和优化
```

**治理框架设计**：
```python
class DataGovernanceFramework:
    def __init__(self, organization):
        self.organization = organization
        self.policies = {}
        self.standards = {}
        self.processes = {}
        self.roles = {}

        # 初始化治理框架
        self.initialize_governance()

    def initialize_governance(self):
        """初始化治理框架"""
        # 定义治理角色
        self._define_governance_roles()

        # 制定治理策略
        self._establish_policies()

        # 定义数据标准
        self._define_data_standards()

        # 建立治理流程
        self._establish_processes()

    def _define_governance_roles(self):
        """定义治理角色"""
        self.roles = {
            'data_steward': {
                'responsibilities': ['数据质量管理', '数据标准制定'],
                'authority': ['数据质量审批', '数据标准变更']
            },
            'data_owner': {
                'responsibilities': ['数据资产管理', '数据安全'],
                'authority': ['数据访问控制', '数据使用审批']
            },
            'data_custodian': {
                'responsibilities': ['数据存储', '数据维护'],
                'authority': ['数据备份', '数据恢复']
            }
        }
```

#### 15.3.2 数据标准管理

**数据标准类型**：
```
数据命名标准：数据对象命名规则
数据格式标准：数据格式规范
数据质量标准：数据质量要求
数据安全标准：数据安全要求
```

**数据标准制定**：
```python
class DataStandardManager:
    def __init__(self):
        self.standards = {}
        self.version_control = StandardVersionControl()

    def create_standard(self, name, standard_type, definition):
        """创建数据标准"""
        standard = {
            'name': name,
            'type': standard_type,
            'definition': definition,
            'version': '1.0',
            'created_at': pd.Timestamp.now(),
            'created_by': 'system',
            'status': 'active'
        }

        self.standards[name] = standard
        self.version_control.save_version(standard)

        return standard

    def update_standard(self, name, new_definition):
        """更新数据标准"""
        if name not in self.standards:
            raise ValueError(f"Standard {name} not found")

        old_version = self.standards[name].copy()

        # 更新标准
        self.standards[name]['definition'] = new_definition
        self.standards[name]['version'] = self._increment_version(old_version['version'])
        self.standards[name]['updated_at'] = pd.Timestamp.now()

        # 保存版本
        self.version_control.save_version(self.standards[name])

        return self.standards[name]

    def validate_data_against_standard(self, data, standard_name):
        """验证数据是否符合标准"""
        if standard_name not in self.standards:
            raise ValueError(f"Standard {standard_name} not found")

        standard = self.standards[standard_name]
        validation_results = []

        # 根据标准类型进行验证
        if standard['type'] == 'naming':
            validation_results.extend(self._validate_naming_standard(data, standard))
        elif standard['type'] == 'format':
            validation_results.extend(self._validate_format_standard(data, standard))
        elif standard['type'] == 'quality':
            validation_results.extend(self._validate_quality_standard(data, standard))

        return validation_results
```

**命名标准**：
```python
class NamingStandard:
    def __init__(self, prefix_rules, suffix_rules, naming_convention):
        self.prefix_rules = prefix_rules
        self.suffix_rules = suffix_rules
        self.naming_convention = naming_convention

    def validate_name(self, name):
        """验证名称是否符合标准"""
        errors = []

        # 验证前缀
        if self.prefix_rules:
            if not any(name.startswith(prefix) for prefix in self.prefix_rules):
                errors.append(f"Name {name} does not follow prefix rules")

        # 验证后缀
        if self.suffix_rules:
            if not any(name.endswith(suffix) for suffix in self.suffix_rules):
                errors.append(f"Name {name} does not follow suffix rules")

        # 验证命名约定
        if self.naming_convention:
            if not re.match(self.naming_convention, name):
                errors.append(f"Name {name} does not follow naming convention")

        return errors
```

#### 15.3.3 数据安全治理

**数据安全原则**：
```
最小权限原则：只授予必要的权限
数据分类：根据敏感度分类数据
访问控制：严格控制数据访问
审计日志：记录所有数据访问
```

**数据分类**：
```python
class DataClassification:
    def __init__(self):
        self.classifications = {
            'public': {
                'description': '公开数据',
                'access_level': 'unrestricted',
                'protection_required': 'minimal'
            },
            'internal': {
                'description': '内部数据',
                'access_level': 'internal_only',
                'protection_required': 'standard'
            },
            'confidential': {
                'description': '机密数据',
                'access_level': 'authorized_only',
                'protection_required': 'high'
            },
            'restricted': {
                'description': '限制数据',
                'access_level': 'highly_authorized',
                'protection_required': 'maximum'
            }
        }

    def classify_data(self, data, classification_rules):
        """对数据进行分类"""
        classification = 'public'  # 默认分类

        for rule in classification_rules:
            if rule['condition'](data):
                classification = rule['classification']
                break

        return classification

    def get_protection_requirements(self, classification):
        """获取保护要求"""
        if classification not in self.classifications:
            raise ValueError(f"Unknown classification: {classification}")

        return self.classifications[classification]['protection_required']
```

**访问控制**：
```python
class AccessControlManager:
    def __init__(self):
        self.access_policies = {}
        self.user_roles = {}
        self.data_permissions = {}

    def define_access_policy(self, policy_name, rules):
        """定义访问策略"""
        self.access_policies[policy_name] = {
            'name': policy_name,
            'rules': rules,
            'created_at': pd.Timestamp.now()
        }

    def check_access_permission(self, user, data_resource, action):
        """检查访问权限"""
        # 获取用户角色
        user_roles = self.get_user_roles(user)

        # 检查每个角色的权限
        for role in user_roles:
            if self.has_permission(role, data_resource, action):
                return True

        return False

    def has_permission(self, role, data_resource, action):
        """检查角色是否有权限"""
        role_permissions = self.data_permissions.get(role, {})

        if data_resource in role_permissions:
            return action in role_permissions[data_resource]

        return False

    def grant_permission(self, role, data_resource, action):
        """授予权限"""
        if role not in self.data_permissions:
            self.data_permissions[role] = {}

        if data_resource not in self.data_permissions[role]:
            self.data_permissions[role][data_resource] = []

        if action not in self.data_permissions[role][data_resource]:
            self.data_permissions[role][data_resource].append(action)
```

#### 15.3.4 数据生命周期管理

**数据生命周期阶段**：
```
创建：数据创建和采集
存储：数据存储和管理
使用：数据使用和分析
归档：数据归档和备份
销毁：数据安全销毁
```

**生命周期管理**：
```python
class DataLifecycleManager:
    def __init__(self):
        self.lifecycle_policies = {}
        self.retention_policies = {}
        self.archival_rules = {}

    def define_lifecycle_policy(self, data_type, stages):
        """定义生命周期策略"""
        self.lifecycle_policies[data_type] = {
            'data_type': data_type,
            'stages': stages,
            'created_at': pd.Timestamp.now()
        }

    def apply_retention_policy(self, data, data_type):
        """应用保留策略"""
        if data_type not in self.retention_policies:
            return

        policy = self.retention_policies[data_type]
        retention_period = policy['retention_period']
        creation_time = policy['creation_time_field']

        # 计算数据年龄
        data_age = (pd.Timestamp.now() - data[creation_time]).dt.days

        # 标记过期数据
        expired_data = data[data_age > retention_period]

        # 处理过期数据
        self._handle_expired_data(expired_data, policy['disposition'])

    def archive_data(self, data, archival_rules):
        """归档数据"""
        for rule in archival_rules:
            if rule['condition'](data):
                self._perform_archival(data, rule['archival_method'])
                break

    def _handle_expired_data(self, data, disposition):
        """处理过期数据"""
        if disposition == 'delete':
            self._secure_delete(data)
        elif disposition == 'archive':
            self._archive_data(data)
        elif disposition == 'anonymize':
            self._anonymize_data(data)

    def _secure_delete(self, data):
        """安全删除数据"""
        # 实现安全删除逻辑
        pass

    def _archive_data(self, data):
        """归档数据"""
        # 实现归档逻辑
        pass

    def _anonymize_data(self, data):
        """匿名化数据"""
        # 实现匿名化逻辑
        pass
```

### 15.4 元数据管理

#### 15.4.1 元数据概念

**元数据定义**：
描述数据的数据，包括数据的结构、内容、质量、关系等信息。

**元数据类型**：
```
技术元数据：数据结构、格式、存储位置
业务元数据：业务含义、业务规则
操作元数据：数据访问、使用记录
管理元数据：数据所有者、质量指标
```

**元数据管理的重要性**：
```
数据发现：帮助用户找到所需数据
数据理解：帮助用户理解数据含义
数据血缘：追踪数据流转过程
数据质量：监控和管理数据质量
```

**元数据架构**：
```python
class MetadataManager:
    def __init__(self):
        self.metadata_store = MetadataStore()
        self.metadata_extractor = MetadataExtractor()
        self.metadata_analyzer = MetadataAnalyzer()
        self.metadata_publisher = MetadataPublisher()

    def collect_metadata(self, data_source):
        """收集元数据"""
        # 提取技术元数据
        technical_metadata = self.metadata_extractor.extract_technical_metadata(data_source)

        # 提取业务元数据
        business_metadata = self.metadata_extractor.extract_business_metadata(data_source)

        # 分析元数据
        analyzed_metadata = self.metadata_analyzer.analyze_metadata(
            technical_metadata, business_metadata
        )

        # 存储元数据
        self.metadata_store.save_metadata(analyzed_metadata)

        return analyzed_metadata

    def update_metadata(self, data_source, changes):
        """更新元数据"""
        # 获取现有元数据
        existing_metadata = self.metadata_store.get_metadata(data_source)

        # 应用变更
        updated_metadata = self._apply_metadata_changes(existing_metadata, changes)

        # 保存更新后的元数据
        self.metadata_store.update_metadata(data_source, updated_metadata)

        return updated_metadata
```

#### 15.4.2 元数据采集

**元数据采集方法**：
```
自动采集：通过工具自动提取元数据
手动录入：人工录入元数据信息
导入导入：从外部系统导入元数据
推理生成：通过数据模式推理元数据
```

**技术元数据采集**：
```python
class TechnicalMetadataExtractor:
    def __init__(self):
        self.extractors = {
            'database': DatabaseMetadataExtractor(),
            'file': FileMetadataExtractor(),
            'api': APIMetadataExtractor()
        }

    def extract_technical_metadata(self, data_source):
        """提取技术元数据"""
        source_type = data_source['type']

        if source_type not in self.extractors:
            raise ValueError(f"Unsupported source type: {source_type}")

        extractor = self.extractors[source_type]
        return extractor.extract(data_source)

class DatabaseMetadataExtractor:
    def extract(self, connection_config):
        """提取数据库元数据"""
        metadata = {
            'database_name': connection_config['database'],
            'tables': [],
            'schemas': [],
            'relationships': []
        }

        # 连接数据库
        connection = self._create_connection(connection_config)

        # 提取表信息
        metadata['tables'] = self._extract_table_info(connection)

        # 提取字段信息
        for table in metadata['tables']:
            table['columns'] = self._extract_column_info(connection, table['name'])

        # 提取关系信息
        metadata['relationships'] = self._extract_relationships(connection)

        return metadata
```

**业务元数据采集**：
```python
class BusinessMetadataExtractor:
    def __init__(self):
        self.business_glossary = BusinessGlossary()
        self.data_dictionary = DataDictionary()

    def extract_business_metadata(self, data_source):
        """提取业务元数据"""
        business_metadata = {
            'business_terms': [],
            'data_dictionary': [],
            'business_rules': [],
            'data_owners': []
        }

        # 从业务术语表提取
        business_metadata['business_terms'] = self._extract_business_terms(data_source)

        # 从数据字典提取
        business_metadata['data_dictionary'] = self._extract_data_dictionary(data_source)

        # 从业务规则文档提取
        business_metadata['business_rules'] = self._extract_business_rules(data_source)

        # 提取数据所有者信息
        business_metadata['data_owners'] = self._extract_data_owners(data_source)

        return business_metadata
```

#### 15.4.3 元数据存储

**元数据存储方案**：
```
关系数据库：传统关系型数据库
图数据库：适合存储关系型元数据
文档数据库：适合存储复杂元数据
专用元数据仓库：专门的元数据存储系统
```

**元数据模型设计**：
```python
class MetadataModel:
    def __init__(self):
        self.entities = {}
        self.attributes = {}
        self.relationships = {}

    def add_entity(self, entity_name, entity_definition):
        """添加实体定义"""
        self.entities[entity_name] = {
            'name': entity_name,
            'definition': entity_definition,
            'attributes': [],
            'relationships': []
        }

    def add_attribute(self, entity_name, attribute_name, attribute_definition):
        """添加属性定义"""
        if entity_name not in self.entities:
            raise ValueError(f"Entity {entity_name} not found")

        attribute = {
            'name': attribute_name,
            'definition': attribute_definition,
            'data_type': None,
            'constraints': []
        }

        self.entities[entity_name]['attributes'].append(attribute)
        self.attributes[f"{entity_name}.{attribute_name}"] = attribute

    def add_relationship(self, source_entity, target_entity, relationship_type):
        """添加关系定义"""
        relationship = {
            'source': source_entity,
            'target': target_entity,
            'type': relationship_type,
            'cardinality': None
        }

        self.entities[source_entity]['relationships'].append(relationship)
        self.relationships[f"{source_entity}_{target_entity}"] = relationship
```

**元数据存储实现**：
```python
class MetadataStore:
    def __init__(self, storage_backend):
        self.storage = storage_backend
        self.cache = MetadataCache()

    def save_metadata(self, metadata):
        """保存元数据"""
        # 保存到存储
        storage_id = self.storage.save(metadata)

        # 更新缓存
        self.cache.put(storage_id, metadata)

        return storage_id

    def get_metadata(self, metadata_id):
        """获取元数据"""
        # 先检查缓存
        cached_metadata = self.cache.get(metadata_id)
        if cached_metadata:
            return cached_metadata

        # 从存储获取
        metadata = self.storage.get(metadata_id)

        # 更新缓存
        if metadata:
            self.cache.put(metadata_id, metadata)

        return metadata

    def search_metadata(self, query):
        """搜索元数据"""
        # 构建搜索条件
        search_criteria = self._build_search_criteria(query)

        # 执行搜索
        results = self.storage.search(search_criteria)

        return results

    def update_metadata(self, metadata_id, updates):
        """更新元数据"""
        # 获取现有元数据
        existing_metadata = self.get_metadata(metadata_id)

        # 应用更新
        updated_metadata = self._apply_updates(existing_metadata, updates)

        # 保存更新后的元数据
        self.storage.update(metadata_id, updated_metadata)

        # 更新缓存
        self.cache.put(metadata_id, updated_metadata)

        return updated_metadata
```

#### 15.4.4 元数据应用

**数据血缘分析**：
```python
class DataLineageAnalyzer:
    def __init__(self, metadata_store):
        self.metadata_store = metadata_store
        self.lineage_graph = LineageGraph()

    def build_lineage_graph(self):
        """构建数据血缘图"""
        # 获取所有数据源
        data_sources = self.metadata_store.get_all_data_sources()

        # 构建图
        for source in data_sources:
            self.lineage_graph.add_node(source['id'], source)

        # 添加数据流关系
        data_flows = self.metadata_store.get_data_flows()
        for flow in data_flows:
            self.lineage_graph.add_edge(
                flow['source'],
                flow['target'],
                flow['transformation']
            )

    def trace_lineage(self, data_asset, direction='upstream'):
        """追踪数据血缘"""
        if direction == 'upstream':
            return self.lineage_graph.get_ancestors(data_asset)
        elif direction == 'downstream':
            return self.lineage_graph.get_descendants(data_asset)
        else:
            raise ValueError("Direction must be 'upstream' or 'downstream'")

    def analyze_impact(self, data_asset):
        """分析变更影响"""
        # 获取下游依赖
        downstream_assets = self.trace_lineage(data_asset, 'downstream')

        impact_analysis = {
            'direct_impact': [],
            'indirect_impact': [],
            'critical_path': []
        }

        for asset in downstream_assets:
            if self._is_directly_impacted(data_asset, asset):
                impact_analysis['direct_impact'].append(asset)
            else:
                impact_analysis['indirect_impact'].append(asset)

        # 计算关键路径
        impact_analysis['critical_path'] = self._calculate_critical_path(data_asset)

        return impact_analysis
```

**数据质量监控**：
```python
class MetadataBasedQualityMonitor:
    def __init__(self, metadata_store):
        self.metadata_store = metadata_store
        self.quality_rules = QualityRuleEngine()

    def monitor_data_quality(self, data_asset):
        """基于元数据监控数据质量"""
        # 获取数据质量元数据
        quality_metadata = self.metadata_store.get_quality_metadata(data_asset)

        # 执行质量检查
        quality_results = []
        for rule in quality_metadata['rules']:
            result = self.quality_rules.execute_rule(rule, data_asset)
            quality_results.append(result)

        # 生成质量报告
        quality_report = {
            'asset_id': data_asset,
            'overall_score': self._calculate_overall_score(quality_results),
            'rule_results': quality_results,
            'trends': self._analyze_quality_trends(data_asset),
            'recommendations': self._generate_recommendations(quality_results)
        }

        return quality_report

    def detect_quality_anomalies(self, data_asset):
        """检测质量异常"""
        # 获取历史质量数据
        historical_data = self.metadata_store.get_historical_quality_data(data_asset)

        # 使用统计方法检测异常
        anomalies = []
        for metric in ['completeness', 'accuracy', 'consistency']:
            metric_values = [record[metric] for record in historical_data]

            if len(metric_values) > 10:
                anomaly_score = self._calculate_anomaly_score(metric_values)
                if anomaly_score > self._get_threshold(metric):
                    anomalies.append({
                        'metric': metric,
                        'score': anomaly_score,
                        'severity': self._determine_severity(anomaly_score)
                    })

        return anomalies
```

**数据发现与推荐**：
```python
class DataDiscoveryEngine:
    def __init__(self, metadata_store):
        self.metadata_store = metadata_store
        self.search_engine = MetadataSearchEngine()
        self.recommendation_engine = RecommendationEngine()

    def discover_datasets(self, search_criteria):
        """发现数据集"""
        # 构建搜索查询
        query = self._build_search_query(search_criteria)

        # 执行搜索
        search_results = self.search_engine.search(query)

        # 排序和评分
        ranked_results = self._rank_results(search_results, search_criteria)

        return ranked_results

    def recommend_datasets(self, user_profile, context):
        """推荐数据集"""
        # 基于用户画像推荐
        user_based_recommendations = self.recommendation_engine.get_user_based_recommendations(
            user_profile
        )

        # 基于上下文推荐
        context_based_recommendations = self.recommendation_engine.get_context_based_recommendations(
            context
        )

        # 基于协作过滤推荐
        collaborative_recommendations = self.recommendation_engine.get_collaborative_recommendations()

        # 合并推荐结果
        combined_recommendations = self._combine_recommendations([
            user_based_recommendations,
            context_based_recommendations,
            collaborative_recommendations
        ])

        return combined_recommendations

    def generate_data_profile(self, dataset_id):
        """生成数据画像"""
        # 获取数据集元数据
        metadata = self.metadata_store.get_dataset_metadata(dataset_id)

        # 分析使用模式
        usage_patterns = self._analyze_usage_patterns(dataset_id)

        # 分析用户群体
        user_demographics = self._analyze_user_demographics(dataset_id)

        # 分析业务价值
        business_value = self._analyze_business_value(dataset_id)

        # 生成完整画像
        data_profile = {
            'basic_info': metadata,
            'usage_patterns': usage_patterns,
            'user_demographics': user_demographics,
            'business_value': business_value,
            'recommendations': self._generate_profile_recommendations(dataset_id)
        }

        return data_profile
```